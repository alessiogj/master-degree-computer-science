\chapter{Crittografia a chiave pubblica}
\section{Crittografia a chiave pubblica}
L'idea di crittografia a chiave pubblica è quella di avere due chiavi, una
pubblica $P_k$ e una privata $S_k$. La chiave pubblica è nota a tutti, mentre
quella privata è nota solo al proprietario. 
Con codifica avviene con la chiave pubblica, mentre la decodifica avviene con la
chiave privata.

Ovviamente l'idea di fondo è avendo in mano il testo cifrato non si riesce a
risalire al testo in chiaro senza la chiave privata. Chiunque può cifrare un
messaggio, ma solo il proprietario della chiave privata può decifrarlo.

In un sistema a chiave pubblica abbiamo i seguenti algoritmi:
\begin{itemize}
    \item Un algoritmo di generazione delle chiavi:
    \[
        \mathcal{G}: 1^k \to (P_k, S_k) 
    \]
    Dove $1^k$ è un parametro che indica la lunghezza della chiave, ovvero il \textit{security parameter}.
    \item Un algoritmo di encription:
      \[
        \mathcal{E}: m, P_k \to \mathcal{E}(m, P_k)
    \]
    \item Un algoritmo di decription:
    \[
          \mathcal{D}: C, S_k \to D(C, P_k)
    \]
\end{itemize}
Ovviamente vale la seguente relazione:
\begin{equation}
    \forall m \quad \mathcal{D}(\mathcal{E}(m, P_k), S_k) = m
\end{equation}
Oltre al fatto che decriptare un messaggio a partire dal testo cifrato senza 
la chiave privata è computazionalmente intrattabile. L'idea è che più la chiave
è lunga più è difficile rompere il sistema. L'idea è che il security parameter
$k$ è proporzionale alla lunghezza della chiave e al crescere di $k$ cresce
la sicurezza del sistema.

Gli algoritmi citati sono algoritmi probabilistici polinomiali. Se voglio 
affermare che tali algoritmi sono polinomiali l'input che rappresenta la 
lunghezza della chiave non può essere 
rappresentato in binario, perché la lunghezza della chiave sarebbe
esponenziale nel numero di bit utilizzati per rappresentare il numero. 
Sulle macchine di Turing la dimensione del problema è data dal numero di 
celle del nastro di input. Utilizzando la teoria della complessità 
basata su tali macchine, o in ogni caso su sistemi dove la dimensione 
dell'input è lo spazio che occupa nella nostra rappresentazione. Per 
voler dire che un algoritmo è polinomiale nel \textbf{valore del 
security parameter} e non nel modo in cui è rappresentato, imponiamo 
che il security parameter sia rappresentato in \textbf{unario}, ovvero 
tanti uni quanti la lunghezza della chiave.

\section{Diffie-Hellman}
Il problema di Diffie-Hellman è il seguente: Alice e Bob vogliono
scambiarsi un segreto senza che Eve lo possa intercettare. Lo strumento 
utilizzato per risolvere il problema è del logaritmo discreto.

Si fissa a priori un numero primo $p$ e un generatore $g$ di $\mathbb{Z}_p^*$.
Un agente A sceglie un numero $x \in_R \{1, \dots, p-1\}$ e calcola $g^x \mod p$. 
Un agente B sceglie un numero $y \in_R \{1, \dots, p-1\}$ e calcola $g^y \mod p$.

\begin{figure}[H]
    \centering
    \begin{tabular}[H]{l|l|l}
        & \textbf{Public} & \textbf{Private} \\
        \hline
        A & $g^x \mod p$ & $x$ \\
        \hline
        B & $g^y \mod p$ & $y$ \\
    \end{tabular}
\end{figure}

A questo punto A e B possono calcolare $g^{xy} \mod p$ e $g^{yx} \mod p$, che coincidono.

Il sistema è sicuro perché calcolare $g^{xy} \mod p$ è computazionalmente intrattabile. Avendo 
a disposizione $g^x$ e $g^y$ non è possibile calcolare $g^{xy}$. Se sappiamo rispondere al problema 
del logaritmo discreto, allora possiamo risolvere il problema di Diffie-Hellman, tale algoritmo 
potrebbe esistere, ma non è stato ancora trovato.

Sia $\mathcal{A} \in \texttt{PPT}$ che calcola $g^{xy} \mod p$ a partire da $g^x \mod p$ e $g^y \mod p$. 
Usiamo $\mathcal{A}$ per costruire un algoritmo $\mathcal{B}$ che risolve il problema del logaritmo discreto,
ma tale dimostrazione non è ancora stata data, perciò non l'algoritmo di Diffie-Hellman non è dimostrabilmente 
sicuro. non siamo in grado di dire che rompere Diffie-Hellman è almeno difficile quanto risolvere il problema
del logaritmo discreto.

\subsection{Ipotesi di Diffie-Hellman}
Qualcuno potrebbe costruire un algoritmo che si basa sull'ipotesi che l'algoritmo di Diffie-Hellman sia sicuro.
\begin{tcolorbox}[title = Ipotesi di Diffie-Hellman]
    Siano $x, y, z$ dei numeri causali scelti in $\{1, \dots, p-1\}$, allora è difficile 
    distinguere ($g^x, g^y, g^{xy}$) da ($g^x, g^y, g^z$).
\end{tcolorbox}

Il concetto di distinguibilità è un concetto probabilistico, ovvero che la possibilità di poter 
distinguere due insiemi di elementi è trascurabile. Ovvero che la probabilità di distinguere 
sia inferiore a $1/2 + \epsilon$, dove $\epsilon$ è trascurabile. Quindi l'attaccante non abbia 
alcun vantaggio rispetto ad un attaccante che non ha alcuna informazione.
Se un attaccante avesse anche un minimo vantaggio, allora potrebbe utilizzare tale vantaggio per
ottenere informazioni sul segreto. Tale vantaggio può essere utilizzato per ottenere l'informazione 
totale mediante esperimenti ripetuti.

\section{Rivest Shamir Adleman - RSA}
$g^x$ è una funzione che è facile da calcolare, ma è difficile da invertire, ovvero calcolare $x$
a partire da $g^x$. Una funzione con questa proprietà è detta \textbf{one-way function}. Il protocollo di 
Diffie-Hellman è sicuro se e solo se esiste una one-way function.
Vorremmo che la one-way function sia anche \textbf{trapdoor}, ovvero che esista un algoritmo 
efficiente che permetta di invertire la funzione. Tale algoritmo è detto \textbf{trapdoor algorithm}.
La trapdoor è una informazione aggiuntiva che permette di invertire la funzione.

Siano $p, q$ due numeri primi molto grandi, $n = pq$. Sia $e$ un numero casuale tale che 
\texttt{mcd}(e, $\varphi(n)$) = 1, dove $\varphi(n) = (p-1)(q-1)$, ovvero il numero di Eulero. Scegliamo 
une elemento $d$ co-primo con $\varphi(n)$ tale che $de \equiv 1 \mod \varphi(n)$.
La chiave pubblica è la coppia $(n, e)$, mentre la chiave privata è la coppia $(n, d)$.
\subsubsection{Algoritmo di cifratura}
\[
  \mathcal{E}: m, (n, e) \mapsto m^e \mod n  
\]
\subsubsection{Algoritmo di decifratura}
\[
  \mathcal{D}: c, (n, d) \mapsto c^d \mod n
\]
\subsection{Funzionamento}
Proviamo a prendere un messaggio $m$ e a cifrarlo con la chiave pubblica $(n, e)$,
e proviamo a decodificarlo con la chiave privata $(n, d)$.
\[
  (m^e)^d = m^{ed} = m^{k\varphi(n) + 1} = m \cdot (m^{\varphi(n)})^k \equiv m \mod n  
\]
Infatti $d\cdot e$ è congruo a 1 modulo $\varphi(n)$, quindi $d\cdot e = k\varphi(n) + 1$.
Per il teorema del resto cinese, $m^{k\varphi(n) + 1} \equiv m$, e non solo per gli elementi 
di $\mathbb{Z}_n^*$.

La funzione one way è la funzione di codifica, quindi $m^e$, l'inversa di 
$m^e$ è la radice $e$-esima, ovvero $m = c^{1/e}$, ma per calcolare la radice $e$-esima
ad oggi non esiste un algoritmo efficiente. L'unico modo per calcolare la radice $e$-esima
è calcolare $d$ e decifrare il messaggio, quindi $d$ è l'informazione trapdoor.

Ci piacerebbe dimostrare che se fossimo in grado di invertire la radice 
e-esima di un numero allora saremmo in grado di fattorizzare $n$, o di calcolare 
il residuo quadratico. Se fosse vero, allora RSA sarebbe sicuro, ma non è stato ancora
dimostrato ad oggi. L'unica sicurezza di RSA è l'esistenza dell'ipotesi di RSA.
\subsection{Attacchi a RSA}
Ci sono casi in cui RSA è stato attaccato, il motivo però era legato alla cattiva 
implementazione dell'algoritmo, e non all'algoritmo in sé.
Quando parliamo di un crittosistema in realtà parliamo di un insieme di algoritmi,
e non di un singolo algoritmo. L'algoritmo di generazione delle chiave impone 
la scelta \textbf{casuale} di $e$ in modo uniforme con gli elementi 
primi con $\varphi(n)$, ma se non fosse casuale, allora potremmo avere dei problemi.
\subsubsection{Attacco sulla base di messaggi piccoli}
Inoltre, in caso di messaggi piccoli, quindi se $m^e < n$, vuol dire che 
non applico nemmeno l'operazione di modulo, e vuol dire in particolare che 
la radice $e$-esima di $m^e$ è una normale radice $e$-esima nell'aritmetica 
dei numeri interi, e quindi è facile da calcolare.
Lavorando con messaggi che a livello numerico sono piccoli, allora 
invertiamo tutto facilmente, più è piccola $e$, più è facile avere 
messaggi che elevati a $e$ sono più piccoli di $n$. Bisogna quindi
star attenti a scenari in cui $m^e < n$.
\subsubsection{Attacco sulla base di messaggi sparsi}
Altri problemi che potrebbe avere RSA sono legati allo spargimento dei 
messaggi.
Immaginiamo di avere un messaggio:

\begin{center}
  \texttt{Buongiorno, il suo voto è 30L}\\
  \texttt{Buongiorno, il suo voto è 30}\\
  \texttt{Buongiorno, il suo voto è 29}\\
  \dots\\
  \texttt{Buongiorno, il suo voto è 0}\\
\end{center}
Uno che vuole decifrare il messaggio può prendere i $32$ messaggi e
cifrarli tutti, per poi distinguerli.
Se con RSA devo codificare messaggi che sono presi da un insieme piccolo,
devo far attenzione perché potrei venir attaccato da qualcuno che utilizza 
la stessa chiave pubblica.
\subsubsection{Attacco sulla informazione parziale}
Siamo sicuri che tutti i bit di questa inversa siano difficili da calcolare?
Non è che sulla radice $e$-esima di $m^e$ ci sono dei bit che sono più facili 
da calcolare? Ai fini di dire che il sistema è sicuro, non è 
sufficiente dire che il cypertext non si sappia ricavare il plaintext,
vorremmo dire che dal cypertext non si riesca a ricavare nessuna informazione 
binaria sul plaintext. Ma come possiamo definire tale proprietà?
\subsubsection{Attacco sulla base di messaggi ripetuti}
Per difendersi da tale problema, aggiungo un po' di rumore al messaggio,
ovvero aggiungo un po' di bit casuali al messaggio, in modo tale che con 
alta probabilità il messaggio non sia mai uguale. In questo modo, anche se
il messaggio è sempre lo stesso, il cypertext è sempre diverso, utilizzando 
quindi la probabilistic encryption.
\section{Sicurezza di un crittosistema}
RSA è sicuro perché non conosciamo un algoritmo probabilistico polinomiale 
per calcolare la radice $e$-esima di un numero. Ma cosa vuol dire?
Se esistesse un algoritmo probabilistico polinomiale per calcolare la radice 
$e$-esima di un numero con probabilità $\frac{1}{k}$, sarebbe un problema,
perché reiterando l'algoritmo $k$ volte, avrei una probabilità di successo.

Se fossimo nello scenario in cui con un $a\in_R \mathbb{Z}_n^*$, l'algoritmo 
mi dia risposta corretta con probabilità $\frac{1}{k}$, potremmo dichiararci 
tranquilli? No, perché potrebbero attaccare sempre.

Fissando $a$ e scegliendo $r \in_R \mathbb{Z}_n^*$, calcolo $(a\cdot r)^e \mod n$,
se prendo un elemento casuale di $\mathbb{Z}_n^*$ e lo elevo ad $e$, ottengo
l'oggetto che è distribuito uniformemente in $\mathbb{Z}_n^*$. Sappiamo che
l'elevamento di $r$ alla $e$-esima è distribuito uniformemente in $\mathbb{Z}_n^*$, 
perché $e$ è stata scelta in maniera tale che la radice $e$-esima dia esattamente 
$r$. Quindi $r^e$ è una funzione invertibile, di conseguenza la funzione che mappa $r$ in 
$r^e$ è una funzione biettiva, quindi un elemento scelto uniformemente in $\mathbb{Z}_n^*$
viene mappato da $r^e$ in un elemento scelto uniformemente scelto in $\mathbb{Z}_n^*$.
Ogni volta che prendiamo un elemento e creiamo una suriezione dello stesso insieme, 
se l'elemento di partenza è scelto uniformemente, il risultato della suriezione, che nella 
sostanza è una permutazione, è scelto uniformemente.

Se prendiamo $r^e$ e lo moltiplichiamo per $a$, otteniamo un elemento che è distribuito
uniformemente in $\mathbb{Z}_n^*$, perché $a$ è scelto uniformemente in $\mathbb{Z}_n^*$, 
poiché la moltiplicazione per $a$ è una funzione biettiva, perché $a$ ammette inverso.

Siamo partiti da un elemento fissato e abbiamo costruito un elemento distribuito uniformemente
e causale, se a quell'elemento applichiamo l'algoritmo della radice $e$-esima, otteniamo
$\frac{1}{k}$ di probabilità di successo, ma se ripetiamo l'algoritmo $k$ volte, abbiamo
una probabilità di successo di $1$, rendendo quindi l'algoritmo indipendente dalla $a$ di 
partenza.
\[
  ar^e \rightarrow \sqrt[e]{ar^e} = r \sqrt[e]{a}
\]
Quindi se prendo il risultato e lo divido per $r$, ottengo $\sqrt[e]{a}$,
che è distribuito uniformemente in $\mathbb{Z}_n^*$, perché $r$ è distribuito uniformemente.

Ed ecco che abbiamo un algoritmo che a partire da una blackbox che con $a$ causale 
calcola correttamente la radice $e$-esima di $a$ una volta su $k$, abbiamo una macchina 
che con $a$ fissato calcola la radice $e$-esima di $a$ con probabilità $1$.

La macchina che calcola la radice $e$-esima di $a$ darà una sequenza di bit, che
potrebbe essere la radice $e$-esima di $a$, oppure no. Bisognerebbe riconoscere la risposta
corretta, rielevando il risultato ad $e$, se ottengo l'input allora la risposta è 
corretta, altrimenti no.

Quindi abbiamo trasformato un algoritmo che funziona una volta su $k$ in un algoritmo
che funziona in un tempo medio di $k$.
\begin{tcolorbox}[title=Definizione di sicurezza]
  Diciamo che un sistema è attaccabile se il tempo medio per attaccarlo è polinomiale.
\end{tcolorbox}
Se esistesse un qualsiasi algoritmo in grado di attaccare la radice $e$-esima di $a$,
con una probabilità polinomiale in $k$, riusciamo a costruire un algoritmo che calcola
la stessa cosa con un tempo medio polinomiale in $k$.

Visto che partiamo  dall'idea che non esista un algoritmo probabilistico polinomiale 
in grado di calcolare la radice $e$-esima di $a$, allora non esiste un algoritmo
che sia in grado di calcolarlo con una probabilità che sia polinomialmente piccola.
Quindi la \textbf{probabilità di successo è più piccola di qualsiasi polinomio}, dove per polinomio 
si intende:
\[
  \probP[\texttt{attacco}] < \frac{1}{k}\quad \forall c
\]
Fissando un polinomio, con chiavi corte, però, la possibilità di trovare un polinomio esiste, 
perciò bisogna correggere tale definizione.
\begin{equation}
  \forall c \exists \bar{k} \forall k > \bar{k}\quad\probP[\texttt{attacco}] < k^{-c}
\end{equation}
Per attacco non intendiamo solo il fatto di non poter essere in grado di poter calcolare la 
radice $e$-esima di $a$, ma anche il fatto di non essere in grado di capire 
\textbf{informazioni binarie}.

L'algoritmo che calcola la radice $e$-esima di $a$ è l'algoritmo che calcola la
la fattorizzazione di $n$, la fattorizzazione di $n$ è l'informazione 
binaria che vogliamo proteggere, ovvero la \textbf{trapdoor} che permette di risolvere 
il problema.
\begin{tcolorbox}[title=Fattorizzazione di $n$]
  Si pensa che non esista un algoritmo \texttt{PPT} che dati $n$, $e$ e $a$,
  calcola $\sqrt[e]{a} \in \mathbb{Z}_n^*$ con probabilità polinomiale.
\end{tcolorbox}
\subsection{Utilizzo pratico di RSA}
Nel caso pratico il costo computazionale di RSA è molto alto, infatti codificare 
un blocco di $k$ bit con RSA richiede $k$ esponenziazioni modulari, ovvero $k^3$, 
un costo computazionale molto alto. Per questo motivo RSA viene utilizzato per
codificare una chiave di sessione, che viene utilizzata per codificare
il messaggio con un algoritmo simmetrico, che è molto più veloce di RSA.
Tipicamente l'algoritmo utilizzato è l'algoritmo simmetrico \texttt{AES} (\ref{section:des}).

Tra l'altro vi è una notevole differenza con Diffie-Hellman, infatti in Diffie-Hellman
riesco a scambiarmi un'unica chiave, a meno che non faccia un nuovo scambio di chiavi,
ogni volta.

Se si parte dall'idea che la crittografia simmetrica sia meno sicura della crittografia
asimmetrica, allora si può pensare di utilizzare una chiave di sessione diversa dopo 
un certo periodo. Con RSA è possibile fare questo, perché è possibile scambiarsi
chiavi diverse, mentre con Diffie-Hellman non è possibile, perché si dovrebbero scambiare
chiavi diverse ogni volta, e questo è molto costoso. La generazione della chiave di sessione 
per Diffie-Hellman è molto costosa, per via delle Certification Authority, che devono
essere coinvolte nel processo di generazione della chiave di sessione per certificare 
le chiavi pubbliche.

\section{Crittosistema di Micali per la codifica di un singolo bit}
\subsubsection{Algoritmo di generazione delle chiavi}
Si sceglie un numero primo $p_1$ e un numero $p_2$ tale che moltiplicati tra loro
diano un numero $n$ tale che $n = p_1p_2$. I due numeri devono essere scelti in modo
casuale con $\frac{k}{2}$ bit ciascuno, dove $k$ è la lunghezza della chiave. Sia 
$y \in_R$ ai non quadrati con simbolo di Jacobi $1$ modulo $n$. Ricordiamo che per 
costruire un numero non quadrato causale con simbolo di Jacobi $1$ basta scegliere un numero
casuale e verificare che il simbolo di Jacobi sia $1$, ovvero che appartenga 
a $\mathbb{Z}_n^*$, se non lo è si sceglie un altro
numero casuale e si ripete il procedimento. A questo punto verifico che il simbolo 
di Legendre rispetto a $p_1$ e $q_1$ sia $-1$. 

La chiave pubblica è:
\[
  P_k = (n, y)
\]
La chiave privata è:
\[
  S_k = (p_1, p_2)
\]
L'ipotesi di base è che sia difficile fattorizzare $n$, ma il problema di 
riferimento sarà il problema del residuo quadratico, ovvero il problema di
calcolare la radice quadrata di un numero modulo $n$.
\subsubsection{Algoritmo di codifica}
L'algoritmo di codifica prende un bit $b$, sia $x \in_R \mathbb{Z}_n^*$, se 
$b$ è $0$ allora $c = x^2 \mod n$, altrimenti $c = xy^2 \mod n$. 
$x^2$ è un quadrato casuale di $\mathbb{Z}_n^*$, mentre $xy^2$ è un 
non quadrato con simbolo di Jacobi $1$. Se prendo un quadrato con simbolo di 
Jacobi $1$ e lo moltiplico per un non quadrato con simbolo di Jacobi $1$ ottengo
un non quadrato con simbolo di Jacobi $1$. Se il quadrato è casuale, allora 
ottengo un non quadrato casuale con simbolo di Jacobi $1$ distribuito uniformemente
tra tutti i non quadrati con simbolo di Jacobi $1$.

Il risultato è che la codifica di $0$ è un quadrato a caso, mentre la codifica di $1$ è
un non quadrato a caso con simbolo di Jacobi $1$.
\[
  \mathcal{E}: \{0, 1\} \to x \in_R \mathbb{Z}_n^*
\]
\[
  f(x) = \begin{cases}
    x^2 \mod n & \text{se } b = 0\\
    xy^2 \mod n & \text{se } b = 1
  \end{cases}
\]
\subsubsection{Algoritmo di decodifica}
L'algoritmo di verifica prende in input $c$ e verifica se $c$ è un quadrato 
rispetto a $p_1$ e $p_2$, ovvero se $c$ è un residuo quadratico modulo $p_1$ e
modulo $p_2$. Se $c$ è un quadrato rispetto a $p_1$ e $p_2$ allora $b = 0$,
se entrambe le verifiche falliscono allora $b = 1$, in altri casi non siamo in presenza 
di un cypertext valido.
\[
  \left(\frac{c}{p_1}\right) = \left(\frac{c}{p_2}\right) = 1 \qquad \text{allora } b = 0
\]
\[
  \left(\frac{c}{p_1}\right) = \left(\frac{c}{p_2}\right) = -1 \qquad \text{allora } b = 1
\]
\subsection{Rompere il crittosistema di Micali}
Rompere tale protocollo significherebbe disporre di un algoritmo $\mathcal{A}$ che preso in 
input in cyphertext $c$ e la chiave pubblica $P_k$ restituisce $b$ con probabilità
diversa da $\frac{1}{2}$, poiché siamo in un contesto binario.

Un attaccante quindi dovrebbe essere in grado di ottenere un vantaggio rispetto 
a qualcuno che non conosce nulla, ovvero che indovini con probabilità $\frac{1}{2}$.
Il vantaggio consiste nell'allontanarsi da $\frac{1}{2}$, sia in positivo che in negativo, 
poiché se si allontana in negativo basta invertire il risultato per ottenere
un vantaggio positivo.

Il numero di esperimenti deve essere tale che la differenza delle probabilità sia
maggiore di $\frac{1}{2}$, ma il numero di esperimenti deve essere un numero 
polinomiale, in modo tale da poter osservare tali esperimenti in tempo polinomiale.
\begin{tcolorbox}[title = Sicurezza]
  \begin{equation}
    \forall c \, \exists \bar{k} \, \forall k \geq \bar{k} \quad \mid \probP[\texttt{Successo}] - \frac{1}{2} \mid  > k^{-c}
  \end{equation}
\end{tcolorbox}
Supponiamo che la probabilità di successo sia maggiore di $\frac{1}{2} + \epsilon$ e vorrei che la probabilità di successo sia quindi 
prossima a $1$. Per farlo eseguo due tipologie di esperimenti, il primo ripete l'esperimento $k$ volte e mediante l'algoritmo che 
ha a disposizione il vantaggio è $\frac{1}{2} + \epsilon$, mentre il secondo esperimento ripete l'esperimento $k$ volte e mediante
esperimenti casuali, ovvero senza l'algoritmo, ottiene un vantaggio di $\frac{1}{2}$. Ripetendo l'esperimento un numero abbastanza grande di 
volte si ottiene il risultato desiderato, poiché basterebbe visualizzare le due distribuzioni per vedere in cosa differiscono. L'algoritmo 
quindi indovina con probabilità $1$. Più $\epsilon$ è piccolo, più esperimenti sono necessari per ottenere il risultato desiderato. 
Servirebbe quindi stimare, dato un $\epsilon$ fissato, il numero di esperimenti necessari per ottenere il risultato desiderato.
\subsubsection{Limite di Chernoff} \label{limite_chernoff}
\begin{tcolorbox}[title = Limite di Chernoff]
  Siano $X_1, \dots, X_n$ variabili casuali e binarie indipendenti con probabilità 
  di successo $\probP[X_i = 1] > \frac{1}{2}$ e $\probP[X_i = 0] = 1 - \probP[X_i = 1]$.
  La probabilità che più della metà delle variabili casuali siano $1$ è:
  \[
    \mathcal{P} = \sum_{i = \frac{n}{2} + 1}^n \binom{n}{i} \probP[X_i = 1]^i \probP[X_i = 0]^{n - i}
  \]
  \[
    \mathcal{P} \geq 1 - e^{-2n\left(p - \frac{1}{2}\right)^2}
  \]
  Tale formula dice che la probabilità che più della metà degli eventi dia $1$ è 
  esponenzialmente vicina a $1$, dove l'eponenzialmente è in funzione di $n$.
  \[
    \probP[\texttt{errore}] = e^{-2 \epsilon n}
  \]
  Dove $\epsilon$ è il vantaggio.
\end{tcolorbox}
Supponiamo di volere $e^{-2 \epsilon n} < \frac{1}{2^k}$, quindi:
\[
  e^{-2c\epsilon^2 n} < 2^{-k}
\]
\[
  -2\epsilon^2 n < c' - k 
\]
\[
  n > \frac{c' - k}{2\epsilon^2}
\]
Se $\epsilon$ è polinomiale in $k$ allora $n$ è polinomiale in $k$.
Di conseguenza, se il vantaggio è polinomiale in qualche security parameter, allora
si riesce ad ottenere una quantità di errore nel security parameter che è
esponenzialmente piccola, scegliendo una quantità di esperimenti polinomiale in esso.

Un sistema è attaccato nel momento in cui esiste un algoritmo polinomiale in 
grado di romperlo.

Nel momento in cui $\epsilon$ è un $k^{-c}$, allora $n$ (\textit{dove $n$ è il numero di
esperimenti}) è polinomiale in $k$.

Visto che l'ipotesi di partenza è che non esistano algoritmi probabilistici polinomiali in grado
di rompere il sistema (\textit{vero}) e visto che abbiamo dimostrato che esiste tale algoritmo (\textit{falso}), allora
l'algoritmo di Micali è sicuro.
\subsubsection{Costruzione degli esperimenti indipendenti}

Una volta capito che la costruzione di $n$ esperimenti indipendenti funziona, bisogna 
capire come costruirli. Supponiamo che la probabilità di successo sia $\frac{1}{2} + \epsilon$ e
di disporre di un algoritmo $\mathcal{A}$ che prende in input un numero $z$ 
con $\left(\frac{z}{n}\right) = 1$, in output restituisce che $z$ è quadrato oppure no.

Per farlo si sceglie $r_1, \dots, r_n \in_R \mathbb{Z}_n^*$ e
si calcola $w_i = z \cdot r_i^2$. L'algoritmo quindi prende in input $w_i$ e restituisce
$b_i$.
\[
  b_i = \mathcal{A}(w_i)
\]

In sostanza si prende in input un numero (\textit{cyphertext}) e l'algoritmo lo moltiplica per una 
quadrato a caso, quindi l'algoritmo restituisce $1$ se il risultato è un quadrato casuale e $0$
altrimenti.

Tale algoritmo però ha un difetto, ovvero che se $z$ è un quadrato, allora $w_i$ è un quadrato
sempre, quindi l'algoritmo restituisce sempre $1$, se invece $z$ non è un quadrato, allora
$w_i$ è un quadrato con probabilità $\frac{1}{2} + \epsilon$.
\subsubsection{Costruzione di un controesempio}
Supponiamo che $\mathcal{A}$ dica correttamente che il $40\%$ dei numeri quadrati sono quadrati e 
che il $62\%$ dei non quadrati sono non quadrati. Sostanzialmente l'algoritmo $\mathcal{A}$
ha una probabilità di successo a seconda dell'input che gli viene dato. 
\[
  \probP[\mathcal{A}(\texttt{n})\texttt{ successo}] = \frac{1}{2} \cdot \frac{40}{100} + 
  \frac{1}{2} \cdot \frac{62}{100} = \frac{51}{100} = 51\%
\]
L'approccio di costruzione degli esperimenti indipendenti non è corretto, perché 
non fornisce all'algoritmo $\mathcal{A}$ un input secondo la misura di probabilità
che $\mathcal{A}$ si aspetta, quando diciamo che ha una certa probabilità di successo.
L'esperimento corretto avviene solamente quando ad $\mathcal{A}$ viene dato un input
un oggetto che sia distribuito uniformemente tra gli oggetti con simbolo di Jacobi $1$.
\subsubsection{Costruzione degli esperimenti indipendenti corretta}
L'idea di base è quella di lanciare una moneta per decidere se invertire o meno la 
quadraticità di $z$ in modo da ottenere un input che sia distribuito uniformemente.
Siano $r_1, \dots, r_n \in_R \mathbb{Z}_n^*$ e siano $s_1, \dots, s_n \in_R \{0,1\}$.
\[
  \forall i \quad w_i = 
  \begin{cases}
    z \cdot r_i^2 & \text{se } s_i = 0 \\
    y \cdot z \cdot r_i^2 & \text{se } s_i = 1
  \end{cases}
\]
Sia $b_i = \mathcal{A}(w_i)$. In questo modo lasciamo una moneta $s_i$ che decide se mantenere 
la quadraticità di $z$ oppure no. Quindi $z \cdot r_i^2$ è un oggetto a caso tra gli oggetti con stessa 
quadraticità di $z$, mentre $y \cdot z \cdot r_i^2$ è un oggetto a caso tra gli oggetti con quadraticità 
opposta di $z$, di conseguenza $w_i$ è un oggetto a caso distribuito
uniformemente tra gli oggetti con simbolo di Jacobi $1$.
In questo caso quindi $\mathcal{A}$ ha una probabilità di successo di $\frac{1}{2} + \epsilon$, ma 
$b_i$ è la risposta al problema trasformato, ma non è la stessa del problema originale.
\[
  \forall i \qquad b_i' = 
  \begin{cases}
    b_i & \text{se } s_i = 0 \\
    \bar{b_i} & \text{se } s_i = 1
  \end{cases}
\]
perché se $s_i = 1$ allora nell'input ho invertito la quadraticità di $z$, di conseguenza la risposta deve essere 
a sua volta invertita per avere una risposta corretta nei confronti di $z$.

Tale costruzione funziona, poiché è possibile passare dall'algoritmo $\mathcal{A}'$
all'algoritmo $\mathcal{A}$, semplicemente invertendo la risposta quando $s_i = 1$, ma non
sempre ciò è attuabile, perché non sempre è possibile invertire la risposta.

L'algoritmo che calcola il residuo quadratico prende in input $z$ e restituisce $1$ se $z$ è un quadrato
e $0$ altrimenti, ma a tale algoritmo abbiamo dato in input $y$, ovvero un non quadrato con simbolo di Jacobi
$1$. Ma siamo davvero capaci di costruire un algoritmo che calcola un non quadrato con simbolo di Jacobi $1$
senza usare la fattorizzazione di $n$? La risposta è no, perché se fosse possibile allora sarebbe possibile
fattorizzare $n$.
\begin{figure}[H]
  \centering
  \begin{tikzpicture}
    % rettangolo con due input e un output
    \node[draw, rectangle, minimum width=4cm, minimum height=3cm] (A) at (0,0) {$\mathcal{A}'$};
    \draw[->] (-5, 0.5) -- (-2, 0.5) node[midway, above] {$z$};
    \draw[->] (-3, -0.5) -- (-2, -0.5) node[midway, below] {$y$};
    \draw[->] (2, 0) -- (5, 0) node[midway, above] {$\{0,1\}$};
    \node[draw, rectangle, minimum width=8cm, minimum height=5cm] (B) at (0,0) {};
    \node (AS) at (-3.2, 2) {$\mathcal{A}''$};
  \end{tikzpicture}
\end{figure}
Sappiamo quindi che la macchina funziona correttamente dato $y$, ma non disponiamo di tale valore.
Perché non prendere un $y$ a caso e verificare se è un quadrato? 

Sia $x \in_R \mathbb{Z}_n^*$ e sia $s \in_R \{0,1\}$, allora 
\[
  w =
  \begin{cases}
    x^2 & \text{se } s = 0 \\
    y \cdot x^2 & \text{se } s = 1
  \end{cases}
\]
Sia $b$ la risposta da utilizzare per l'algoritmo. Se per puro caso però $y$ fosse un quadrato, non riuscirei 
a cambiare la quadraticità di $w$ nel caso $s = 1$. Quindi nel caso in cui $s$ fosse 
uguale a $1$ e $y$ fosse un quadrato, allora $w$ sarebbe un quadrato, quindi la risposta dell'algoritmo sarebbe
sempre sbagliata, poiché verrebbe complementata la risposta pensando che $w$ non sia un quadrato. In ogni caso 
però è possibile sorvolare tale problema, poiché basta vedere come si comporta statisticamente 
la macchina in presenza di non quadrati e di quadrati. Il comportamento della macchina è per forza diverso di fronte 
alle due situazioni, perché altrimenti non sarebbe capace di distinguere i due input.
\section{Distinguisher}\label{sec:distinguisher}
Codificare $b_1, \dots, b_l$ in $E(b_1), \dots, E(b_l)$ può essere fatto facendo si che dal testo cifrato non si risalga 
al testo in chiaro. Dimenticandoci del problema della malleabilità, vorremmo che dal cyphertext non si possa risalire
ad alcuna informazione binaria sul plaintext, ma dobbiamo capire cosa vuol dire non poter risalire ad alcuna informazione
binaria.
\begin{tcolorbox}[title = Distinguisher]
  Un \textbf{distinguisher} è un algoritmo binario $\mathcal{D} \in \texttt{PPT}$ che restituisce $0$ o $1$. Il distinguisher
  prende in input un evento che soddisfa o meno una certa proprietà e 
  un altro evento che soddisfa o meno la stessa proprietà. Il distinguisher deve essere in grado di distinguere
  se i due eventi si comportano in modo diverso o meno.
  Statisticamente il distinguisher verifica che i due eventi si comportino in modo polinomialmente diverso
  (\ref{limite_chernoff}), altrimenti tale differenza non sarebbe distinguibile.
\end{tcolorbox}
Sia $\probP_k^{\mathcal{D}, m}$ la probabilità che il distinguisher $\mathcal{D}$ restituisca $1$ su input, una codifica 
di $m$ quando il security parameter vale $k$. Un sistema di codifica nascone $m_1$ e $m_2$ a $\mathcal{D}$ quando:
\begin{equation}
  \forall c \, \exists \bar{k} \, \forall k \geq \bar{k} \qquad
  |\probP_k^{\mathcal{D}, m_1} - \probP_k^{\mathcal{D}, m_2}| < k^{-c}
\end{equation}
Ovvero:
\begin{equation}
  |\probP_k^{\mathcal{D}, m_1} - \probP_k^{\mathcal{D}, m_2}| < k^{-\omega(1)}
\end{equation}
$E$ nasconde informazioni a $\mathcal{D}$ se per ogni $m_1$ e $m_2$ $E$ nasconde $m_1$ e $m_2$ a $\mathcal{D}$. 
$E$ nasconde se per ogni $\mathcal{D} \in \texttt{PPT}$ $E$ nasconde $m_1$ e $m_2$ a $\mathcal{D}$.
\begin{proof}
  Supponiamo per assurdo che $\exists \mathcal{D} \in \texttt{PPT}$ tale che 
  \[
    \exists m_1, m_2, \,\forall \bar{k} \, \exists k \geq \bar{k} \qquad
    |\probP_k^{\mathcal{D}, m_1} - \probP_k^{\mathcal{D}, m_2}| \geq k^{-c}
  \]
   Allora $\mathcal{D}$ può distinguere su due messaggi $m_1$ e $m_2$ che differiscono di un solo bit.
   \begin{equation*}
    m_1 = \alpha^1 \alpha^2 \dots \alpha^l \qquad
    m_2 = \beta^1 \beta^2 \dots \beta^l 
   \end{equation*}

Definisco $\forall i \in \{0,\dots,l\} \,m(i) = \beta^1 \dots \beta^{i-1} \beta^i \alpha^{i+1} \dots \alpha^l$, 
ovvero una serie di bit intermedi per poter passare da $m_1$ a $m_2$ variando un solo bit per volta.
Quindi $m(0) = m_1$, $m(l) = m_2$, di conseguenza $\probP_k^{\mathcal{D}, m(i)} = \probP(i)$, ovvero la probabilità 
che il distinguisher restituisca $1$ se viene data in input una codifica del messaggio $m_i$.
  \[
    \probP(0) = \probP_k^{\mathcal{D}, m_1} \qquad \probP(l) = \probP_k^{\mathcal{D}, m_2}
  \]
Scopriamo quindi che l'ipotesi diventa:
\[
  k^{-c} \leq |\probP(0) - \probP(l)| = |\sum_{i=0}^{l - 1} \probP(i) - \probP(i+1)| 
  \leq \sum_{i=0}^{l - 1} |\probP(i) - \probP(i+1)|
\]
Poiché sappiamo che $\probP(0) - \probP(1) + \probP(1) - \probP(2) + \dots + \probP(l-1) - \probP(l) = \probP(0) - \probP(l)$. 
Quindi:
\[
  \sum_{i=0}^{l - 1} |\probP(i) - \probP(i+1)| \geq k^{-c}
\]
Avendo la somma di numeri che eccede un determinato valore allora so che esiste almeno un elemento che è maggiore o uguale 
della media.
\[
  \exists i \in \{0,\dots,l-1\}\qquad \textit{t.c.} \qquad |\probP(i) - \probP(i+1)| \geq \frac{k^{-c}}{l} \geq k^{-(c+1)} = k^{-c'}
\]
Abbiamo quindi trovato un polinomio $c'$ tale per cui il distinguisher è in grado di distinguere due messaggi che differiscono
di un solo bit.
I due messaggi $m_1$ e $m_2$ differiscono di un solo bit, quindi:
\begin{align*}
  \begin{array}{r c lllllllll}
  m(i) & = & \beta^1 &\dots &\beta^{i-1} & \beta^i &\textcolor{red}{\alpha^{i+1}} & \alpha^{i+2} &\dots &\alpha^l \\
  m(i+1) & = & \beta^1 &\dots &\beta^{i-1} & \beta^i &\textcolor{red}{\beta^{i+1}} & \alpha^{i+2} &\dots &\alpha^l \\
  \end{array}
\end{align*}
A questo punto supponiamo senza perdita di generalità che $\alpha^{i+1} = 0$ e $\beta^{i+1} = 1$. Sia $z$ un 
elemento di $\mathbb{Z}_n^*$ con $\left( \frac{z}{n} \right) = 1$, quindi $z$ potrebbe essere la codifica di uno 
$0$ o di un $1$.

Dato $z$ viene costruito $E(\beta_1)E(\beta_2)\dots E(\beta_i)zE(\alpha_{i+2})\dots E(\alpha_l)$ e viene lanciato 
l'algoritmo $\mathcal{D}$ sul risultato. La probabilità con cui $\mathcal{D}$ restituisce $1$ è \probP(i) se $z$
codifica $0$ e $\probP(i+1)$ se $z$ codifica $1$ se in input viene data la codifica di $m(i)$ secondo 
l'algoritmo per codificare il messaggio $m_i$, ovvero applicando $E$ a tutti i bit del messaggio.
Avendo però aggiunto $z$ in mezzo al messaggio non ho codificato secondo l'algoritmo che il distinguisher
si aspetta, quindi bisogna codificare il messaggio $m(i)$ in maniera corretta:
\[
  E(\beta_1)E(\beta_2)\dots E(\beta_i)\,\textcolor{red}{z \cdot x^2}\,E(\alpha_{i+2})\dots E(\alpha_l)\qquad \text{con } 
  x \in \mathbb{Z}_n^*
\]
A questo punto il distinguisher restituisce $1$ con probabilità $\probP(i)$ se $z$
codifica $0$ e $\probP(i+1)$ se $z$.

Chiamiamo $\mathcal{P}_0$ la probabilità che il distinguisher restituisca $0$ (\textit{quindi $\probP(i)$}) e 
$\mathcal{P}_1$ la probabilità che il distinguisher restituisca $1$ (\textit{ovvero $\probP(i+1)$}) se in input dato è costruito come sopra.

Se si riceve la codifica di un bit scelto a caso, con quale probabilità si riesce a distinguere se è $0$ o $1$?
Si utilizza il risultato del distinguisher $\mathcal{D}$ come tentativo.
\[
  \probP[\texttt{indovinare}] = \frac{1}{2} \cdot (1 - \mathcal{P}_0) + \frac{1}{2} \cdot \mathcal{P}_1 = 
  \frac{1}{2} + \frac{1}{2} \cdot (\mathcal{P}_1 - \mathcal{P}_0)
\]
Se sottraiamo $\frac{1}{2}$ otteniamo:
  \begin{align*}
    \probP[\texttt{non indovinare}] &= \frac{1}{2} + \frac{1}{2} \cdot (\mathcal{P}_1 -
  \mathcal{P}_0) - \frac{1}{2} \\
  &= \frac{1}{2} \cdot (\mathcal{P}_1 - \mathcal{P}_0) \\
  &= \frac{1}{2} \cdot |\probP(i) - \probP(i+1)| \\
  &\geq \frac{1}{2} \cdot k^{-c'}
  \end{align*}
L'algoritmo ha quindi un vantaggio di almeno $\frac{1}{2} \cdot k^{-c'}$, quindi tale algoritmo è 
un attaccante.
\end{proof}
%\subsection{Distinguere e indovinare}
%TODO
\section{Lancio della moneta}
Il sistema descritto in precedenza ha un problema, infatti sappiamo che 
la codifica di un singolo bit richiede una quantità molto alta di bit. Infatti 
implicherebbe un enorme spreco di banda, di risorse. Ci piacerebbe giungere 
ad uno scenario in cui un singolo bit viene codificato con un solo bit.

Per arrivare a questo punto impareremo a costruire numeri \textbf{pseudo-casuali
crittograficamente sicuri} da utilizzare come base per simulare un one-time pad, 
dove la chiave non è realmente casuale, ma è una chiave generata da noi, 
ma nessuno con potenza di calcolo polinomiale possa realmente notare la 
differenza.

Il lancio di monete in rete è un problema molto importante, infatti simulare 
un reale lancio di monete è molto difficile, in quanto non si può avere
la certezza che uno dei due agenti non stia mentendo.
Disponiamo dei due agenti $A$ e $B$ che vogliono simulare un
lancio di moneta e che comunicano attraverso un canale di comunicazione
e vorremmo che il lancio avvenga in modo simile a quello reale, ovvero
che entrambi gli agenti non sappiano il risultato del lancio prima che
questo avvenga e che il risultato sia frutto del lancio di una moneta.

Il fatto che $A$ lanci la moneta e che comunichi il risultato a $B$ non
va bene, poiché il risultato può essere manipolato da $A$ affinché sia 
di suo gradimento, e l'altro agente non può avere la certezza che il
risultato sia reale.
Si potrebbe provare a comunicare il risultato in contemporanea, ma anche
in questo caso si potrebbe simulare un ritardo di rete in modo da 
sfruttare il risultato a proprio vantaggio.
Ci si potrebbe affidare ad un terzo agente, ma anche in questo caso
l'agente esterno deve essere fidato.

Abbiamo quindi dato due risultati al lancio della moneta, ovvero
$Coin_a$ e $Coin_b$. $Coin_a$ è una variabile del processo locale dell'agente
$A$ e $Coin_b$ è una variabile del processo locale dell'agente $B$, ed entrambe
conterranno il risultato del lancio della moneta. Se $A$ e $B$ seguono 
il protocollo (\textit{sono onesti}), allora $Coin_a = Coin_b$, con 
\[
  \probP[Coin_a = 0] = \frac{1}{2}
\]
Potrebbero però non seguire il protocollo e indurre l'altro agente in errore, ottenendo 
un risultato iniquo, nel nostro caso $A$ segue il protocollo, mentre $B$ no, non possiamo 
fornire alcuna garanzia su $B$, possiamo però tutelare $A$. Per tutelarlo, 
$A$ deve essere in grado di osservare il risultato di un lancio di una 
moneta.
\[
  | \probP[Coin_a = 0] - \frac{1}{2} | < k^{-\omega(1)}
\]
Ammettiamo che $B$ possa alterare tale probabilità, ma non la può alterare di troppo,
quindi la differenza tra la probabilità reale e quella che $B$ può alterare deve essere
minore di qualsiasi polinomio. Per ogni esponente esiste un valore minimo della 
lunghezza della chiave tale per cui con chiavi sufficientemente lunghe, 
la probabilità si discosta da $\frac{1}{2}$ di un valore minore di $k^{-\omega(1)}$.
\subsection{Lancio della moneta con il residuo quadratico}
Ci basiamo sulla difficoltà di stabilire se un numero è un quadrato o meno in $\ZZ_p^*$.
\pseudocodeblock{
\begin{tikzpicture}
  \node[businessman, monitor, female, minimum size=2cm,label=below:{Alice}] (alice) {};
\end{tikzpicture}
\<\<
\begin{tikzpicture}
  \node[dave, monitor, mirrored, minimum size=2cm, label=below:{Bob}] (bob) {};
\end{tikzpicture}
\\[0.1\baselineskip][\hline] \\[-0.5\baselineskip]
\text{Sceglie }p,q \in_R primi \\
\text{Calcola } n=p\cdot q \\
\text{Calcola } z \in \ZZ_n^* \text{ con } \left(\frac{z}{n}\right)=1\\
\< \sendmessageright*{n,z} \\
\<\< \text{Sceglie } b \in_R \{0,1\} \\
\< \sendmessageleft*{b} \\
\< \sendmessageright*{p, q} \\
}

Il risultato finale sarà $b \oplus \texttt{isSquare}(z)$. Sostanzialmente 
$A$ lancia una moneta (\textit{sceglie a caso fra un quadrato o un non quadrato, poiché 
scegliendo uniformemente fra gli elementi di $\ZZ_n^*$ con simbolo 
di Jacobi $1$, si ottiene un quadrato con probabilità $\frac{1}{2}$}) e
$B$ sceglie un bit $b$ a caso e lo invia ad $A$. Quando $B$ riceve $z$ non 
sa qual è il risultato del lancio della moneta, perché non sa risolvere il 
problema del residuo quadratico, di conseguenza è vero che $A$ invia prima 
il risultato del proprio lancio della moneta, ma nella condizione in cui $B$ non 
vede il risultato, ma $B$ non invia il bit $b$ in funzione di $z$.

Se entrambi gli agenti sono onesti, $z$ è stato campionato secondo 
una misura causale, quindi $z$ è un quadrato con probabilità $\frac{1}{2}$ 
e un non quadrato con probabilità $\frac{1}{2}$. Anche $b$ è stato
campionato secondo una misura causale, quindi $b$ è $0$ con probabilità
$\frac{1}{2}$ e $1$ con probabilità $\frac{1}{2}$. Alla fine del protocollo,
tutti sono in grado di calcolare il risultato perché tutti conoscono 
gli interi parametri dell'algoritmo, che sarà uguale per entrambi seguendo la
formula: $b \oplus \texttt{isSquare}(z)$.

Se $B$ è disonesto, allora può calcolare il bit $b$ non casuale, distribuendolo
in maniera differente. Se $B$ calcola $b$ \textbf{indipendentemente} da 
$z$, allora qualunque sia la regola di calcolo di $b$, sarà sempre 
uguale alla quadraticità di $z$ con probabilità $\frac{1}{2}$. Un altro modo 
per imbrogliare è quello di calcolare $b$ in funzione di $z$, ma 
questo richiede la conoscenza dell'algoritmo della quadraticità di $z$.
$B$ ha il vantaggio di allontanarsi da $\frac{1}{2}$, ma è il vantaggio 
pari a quello della risoluzione del problema del residuo quadratico, quindi 
più piccolo di qualsiasi polinomio.

Se $A$ è disonesto, può calcolare $p$ e $q$ non primi, ma $B$, alla fine del 
protocollo se ne accorgerebbe ricevendo $p$ e $q$ e quindi potrebbe
lanciare una propria moneta per decidere il proprio risultato o, se in conoscenza 
del risultato che gli è favorevole, può decidere come risultato un valore che gli 
fornirebbe tale vantaggio. 
$A$ potrebbe non inviare $p$ e $q$, in questo caso $B$ ha un enorme 
svantaggio, perché dopo il secondo messaggio $A$ conosce $Coin_A$, ma 
$B$ non conosce $Coin_B$, se $A$ invia il proprio messaggio a $B$, $B$ 
è in grado di calcolare $Coin_B$ e quindi il risultato finale, ma se $A$
sa il risultato vincente e il risultato ottenuto è sfavorevole, $A$ non 
spedisce il terzo messaggio e $B$ lancia la propria moneta che con probabilità 
$\frac{1}{2}$ è favorevole, ma con probabilità $\frac{1}{2}$ è sfavorevole,
ma si crea una situazione sfavorevole a $B$
\[
  \probP[Coin_B = 0] = \frac{1}{2} + \frac{1}{2} \cdot \frac{1}{2} = \frac{3}{4}
\]
Il comportamento di $A$ ha alterato la misura di probabilità di successo di $B$, a 
favore di $A$.

Il protocollo appena descritto non soddisfa le proprietà di un protocollo di lancio della 
moneta. Ci sono casi in cui il protocollo però può essere seguito, ovvero quando chi 
impersona $A$ non conosce il risultato vincente e $B$ può o non può conoscere il risultato 
vincente.
\begin{tcolorbox}
  Un protocollo per funzionare deve essere tale che gli agenti possano conoscere il 
  risultato anche senza l'ultimo messaggio. Se l'ultimo messaggio è quello significativo,
  allora il protocollo non è corretto.
\end{tcolorbox}
Sia $n$ il numero minimo di messaggi di un protocollo che funziona, quindi gli agenti conoscono 
il risultato del lancio della moneta anche senza l'invio dell'ultimo messaggio.
ciò vuol dire che l'ultimo messaggio non serve, ma se non serve allora il penultimo 
messaggio è quello che permette ad $A$ di conoscere il risultato, ma se $B$ decide di non 
inviare tale messaggio si innesca una reazione a catena che porta $A$ e $B$ a non inviare nulla.
\begin{tcolorbox}
Non esiste il protocollo per il lancio della moneta che soddisfi le condizioni dettate da noi.
Il protocollo funziona solo se, con $n$ agenti, ad imbrogliare sono meno di $\frac{n}{2}$ agenti.
\end{tcolorbox}
Indeboliamo la soluzione in modo che funzioni.
\subsubsection{Lancio di moneta nel pozzo}
L'idea è quella di utilizzare un pozzo, in cui finirà la moneta. $A$ riesce a vedere il risultato
del lancio della moneta, ma $B$ no, perché si trova di intralcio un cane tenuto 
al guinzaglio da $A$. $A$ può decidere far si che $B$ possa vedere il risultato, facendo rientrare 
il cane che si trova nei pressi del pozzo, ma $A$ può anche decidere di non far rientrare il cane.

$A$ non può variare il risultato del lancio della moneta, ha solamente la facoltà di impedire o meno 
a $B$ di vedere il risultato. Sa a $B$ decidere se partecipare al processo o meno, non gli interessa
il risultato del lancio della moneta, ma interessa solamente che la probabilità del lancio sia di 
almeno $\frac{1}{2}$.
\subsubsection{Oblivious transfer}
Supponiamo che $B$ sappiamo il risultato della borsa del prossimo anno e che chieda a ad $A$ di 
avere in cambio del denaro per i risultati della borsa. $B$ non vuole dare tutto il denaro ad $A$,
ma vuole dare solamente una parte del denaro. A questo punto $A$ decide di far il lancio della moneta
per decidere se mostrare o meno il risultato della borsa a $B$. $B$ a questo punto deve trasferire 
l'informazione ad $A$, ma \textbf{non gli interessa se l'informazione è stata trasferita o meno}, ma solamente
che la probabilità che l'informazione sia stata trasferita sia di almeno $\frac{1}{2}$.
\pseudocodeblock{
\begin{tikzpicture}
  \node[businessman, monitor, female, minimum size=2cm,label=below:{Alice}] (alice) {};
\end{tikzpicture}
\<\<
\begin{tikzpicture}
  \node[dave, monitor, mirrored, minimum size=2cm, label=below:{Bob}] (bob) {};
\end{tikzpicture}
\\[0.1\baselineskip][\hline] \\[-0.5\baselineskip]
\<\<\text{Sceglie }p,q \in_R primi \\
\<\<\text{Calcola } n=p\cdot q \\
\<\<\text{Calcola } e, d \text{ chiave } p \text{ e } p  \text{ per } \texttt{RSA} \\
\<\<\text{Codifica il segreto con \texttt{RSA} ottenendo} \\
\<\<\text{un messaggio } c \\
\<\sendmessageleft*{n,e,c} \\
\text{Sceglie } x \in_R \ZZ_n^* \\
\<\sendmessageright*{x^2} \\
\<\<\text{Calcola una radice quadrata di } x, \text{ che sarà } y \\
\<\sendmessageleft*{y} \\
}
Quindi $A$ invia un quadrato a caso e $B$ invia una delle quattro radici quadrate di $x$.
Con probabilità $\frac{1}{2}$ tale radice sarà diversa da $\pm x$. Quando un agente possiede 
due radici quadrate di uno stesso numero \textbf{che non sono una l'opposta dell'altro}, allora 
è in grado di fattorizzare $n$.
Con probabilità $\frac{1}{2}$, $B$ invierà ad $A$ una radice diversa da $\pm x$, metterà quindi 
$A$ nelle condizioni di calcolare $p$ e $q$ e quindi $d$ per decodificare il messaggio $c$.
$B$ non ha alcun interesse nel risultato, che otterrà con probabilità $\frac{1}{2}$, ma
interessa che $A$ abbia sia riuscita a decifrare $c$ con probabilità $\frac{1}{2}$.

Il vantaggio di $A$ è implicito nel risultato.
\subsection{Lancio della moneta con il logaritmo discreto}
Il problema è che fino ad ora abbiamo lavorato con la quadraticità di un numero 
in $\ZZ_n^*$, ma vorremmo lavorare con singoli bit. Quindi non con l'inversa di una 
funzione one way, ma con singoli bit, ovvero con l'informazione \textbf{binaria} 
dell'inversa di una funzione one way.

Disponiamo di un numero $p$ e un generatore $g$ di $\ZZ_p^*$, quindi $g$ è un numero
che genera tutti gli elementi di $\ZZ_p^*$, ovvero $\{g^0, g^1, \ldots, g^{p-1}\}$.
I due valori $p$ e $g$ sono condivisi tra $A$ e $B$, prima dell'inizio del protocollo.
\pseudocodeblock{
\begin{tikzpicture}
  \node[businessman, monitor, female, minimum size=2cm,label=below:{Alice}] (alice) {};
\end{tikzpicture}
\<\<
\begin{tikzpicture}
  \node[dave, monitor, mirrored, minimum size=2cm, label=below:{Bob}] (bob) {};
\end{tikzpicture}
\\[0.1\baselineskip][\hline] \\[-0.5\baselineskip]
\text{Sceglie }x \in_R \{0,\dots,p-2\} \\
\text{Calcola } z = g^x \\
\<\sendmessageright*{z} \\
\<\<\text{Sceglie } b \in_R \{0,1\} \\
\<\sendmessageleft*{b} \\
\<\sendmessageright*{x} \\
}
Il risultato è quindi $b \oplus \left( x < \frac{p-1}{2}\right)$. L'idea di fondo è che $B$ 
quando sceglie $b$ non è in grado di dedurre il risultato del lancio della moneta, perché
dovrebbe verificare se $x$ è minore o meno di $\frac{p-1}{2}$, facendo quindi un test binario 
su il \textbf{logaritmo discreto} di $z$. Nel momento in cui $A$ invia $x$, $B$ è in grado
di verificare la correttezza del risultato e a calcolare il risultato.

\begin{tcolorbox}
  $B$ non è in grado di calcolare il predicato binario $x < \frac{p-1}{2}$, a meno di un 
  vantaggio più piccolo di qualsiasi polinomio.
\end{tcolorbox}
Tale algoritmo utilizza un \textbf{predicato binario sull'inversa di una funzione one way}.
Data una funzione one way $f$, un predicato binario $P$ sull'inversa di $f$, non siamo 
certi che il predicato sia difficile da calcolare. 

Per il logaritmo discreto, il predicato binario facile da calcolare è il \textbf{bit 
meno significativo} di x. Tale bit vale zero se e solo se $z$ è un quadrato in $\ZZ_p^*$.
Ma tale predicato dice qualcosa di differente, ovvero ci chiediamo se il logaritmo discreto di 
$z$ sta nella prima metà o nella seconda metà dei logaritmi discreti di $\ZZ_p^*$ o nella seconda,
ovvero una sorta di \textbf{bit più significativo} di $x$.
\section{Hardcore predicate del logaritmo discreto}
\begin{tcolorbox}[title = Hardcore predicate]
  Si tratta di un predicato binario sull'inversa di una funzione one way, che è difficile da
  calcolare. Se scegliessimo a caso il valore di tale predicato binario, la 
  probabilità con cui un algoritmo riesce a calcolare il predicato conoscendo 
  $z$ è $\frac{1}{2} + \epsilon$, con $\epsilon$ più piccolo di qualsiasi 
  polinomio.
\end{tcolorbox}
Sia $y = g^x$ e sia il predicato in considerazione $x < \frac{p - 1}{2}$. Se 
  $y$ è un quadrato, allora ammette due radici quadrate, $z_1$ e $z_2$,
  di queste due radici, una ha logaritmo discreto minore di $\frac{p-1}{2}$ e l'altra
  maggiore o uguale a $\frac{p-1}{2}$. Questo perché le rispettive radici quadrate
  sono nella forma $g^{i}$ e $g^{i + \frac{p-1}{2}}$. La radice quadrata 
  nella forma $g^{i}$ è chiamata \textbf{radice quadrata principale}.

  Di base se abbiamo due radici quadrate di $y$, che sappiamo 
  calcolare aritmeticamente, non sappiamo come capire quale sia
  la radice quadrata principale, ma se 
  avessimo a disposizione l'algoritmo per calcolare se il logaritmo 
  discreto di un numero è minore di $\frac{p-1}{2}$, allora potremmo
  calcolare la radice quadrata principale di un quadrato.

  Supponiamo di avere un algoritmo che date le due radici quadrate di un numero 
  distingue la radice quadrata principale. In questo caso possiamo costruire
  un algoritmo che calcola il predicato binario $x < \frac{p-1}{2}$.
  \begin{theorem}
    Qualunque funzione one-way ammette un hardcore predicate.
  \end{theorem}
  Ne deriva quindi che:
  \begin{theorem}
  Se esiste un algoritmo per il calcolo della radice principale 
  di un quadrato in $\ZZ_p^*$, allora esiste un algoritmo efficiente
  per il calcolo del logaritmo discreto in $\ZZ_p^*$.
\end{theorem}
\begin{theorem}
  Se esiste che riesce  a calcolare il predicato binario con vantaggio 
  polinomiale rispetto al caso casuale, allora esiste un algoritmo
  probabilistico polinomiale per il calcolo del logaritmo discreto.
\end{theorem}
Per dimostrare il teorema dovremmo seguire tre passaggi:
\begin{enumerate}
  \item Mostrare che siamo in grado di calcolare il logaritmo discreto
  di un numero avendo in mano un algoritmo per il calcolo della radice
  quadrata principale di un quadrato.
  \item Mostrare che tale algoritmo funziona anche se la 
  radice quadrata principale è calcolata con probabilità 
  esponenzialmente vicina a $1$.
  \item Mostrare che partendo da un algoritmo che funziona con vantaggio 
  polinomiale rispetto al caso casuale, siamo in grado di costruire un
  algoritmo che funziona con probabilità esponenzialmente vicina a $1$.
\end{enumerate}
\subsection{Calcolare il logaritmo discreto avendo a disposizione
l'algoritmo per il calcolo della radice quadrata principale (\texttt{PSQR})}
Avremo a disposizione l'algoritmo \texttt{PSQR} che calcolerà la radice
quadrata principale di un quadrato in $\ZZ_p^*$, avendo a disposizione 
il predicato binario. Per farlo prende in input $y$ ne calcola la radice 
quadrata, verifica il predicato binario per capire se è la radice quadrata
principale, se la verifica fallisce, calcola la radice principale 
opposta e restituisce il risultato.

\begin{algorithmic}[1]
  \Procedure{LSB}{$y$}
    \If{$\frac{y}{p} = 0$}
      \State \Return $0$
    \Else
      \State \Return $1$
    \EndIf
  \EndProcedure
\end{algorithmic}

Ricordiamo che la procedura $\texttt{LSB}$ verifica se il bit meno significativo
di un numero è pari o dispari. Se è pari restituisce $0$, altrimenti
restituisce $1$.

\begin{algorithmic}[1]
  \Procedure{DiscreteLogarithm}{$y$}
    \If{$y = 1$} 
      \State \Return $0$ \EndIf
    \State $b \gets \texttt{LSB}(DiscreteLogarithm(y))$
    \If{$b = 1$}
      \State \Return $y  \gets y \cdot g^{-1}$ \Comment{imposta a zero il bit meno significativo}
    \EndIf
    \State $y \gets \texttt{PSQR}(y)$ \Comment{scorri a destra di un bit}
    \State \Return $2 \cdot \texttt{DiscreteLogarithm}(y) + b$
  \EndProcedure
\end{algorithmic}
Per calcolare la radice quadrata abbiamo necessariamente bisogno del bit meno significativo
a zero. 

L'algoritmo ricorsivo permette di calcolare il logaritmo discreto di un numero calcolando 
il bit meno significativo e riconducendo il calcolo dello stesso problema 
in una situazione in cui si ha un bit in meno.

\begin{tcolorbox}
  Se abbiamo un algoritmo che funziona per il calcolo della radice quadrata principale
  allora possiamo costruire un algoritmo che calcola il logaritmo discreto.
\end{tcolorbox}

Supponiamo di avere un algoritmo $B$ per \texttt{PSQR} che funziona con probabilità 
esponenzialmente vicina a $1$, ovvero $1 - \epsilon$. La probabilità che l'algoritmo
\texttt{DiscreteLogarithm} invocato $k$ volte fornisca sempre la risposta corretta
è $(1 - \epsilon)^{k}$. L'algoritmo \texttt{DiscreteLogarithm} può non funzionare, 
ma abbiamo modo di accorgerci:
\begin{itemize}
  \item Se il numero di passi è maggiore del numero di bit di $y$, allora
  siamo sicuri che il \texttt{PSQR} non ha funzionato.
  \item Sul risultato finale possiamo verificare che $g^x = y$. Se non è così
  allora \texttt{PSQR} non ha funzionato.
\end{itemize}
In media se funziona con probabilità $(1 - \epsilon)^k$, allora il numero di volte
che funziona in cui si dovrà lanciare l'algoritmo sarà 
$\frac{1}{(1 - \epsilon)^k}$.
Se $\epsilon \leq \frac{1}{2}$, allora $(1 - \epsilon)^k \geq \frac{1}{2^k}$.
\begin{tcolorbox}
  Se abbiamo un algoritmo che funziona con probabilità esponenzialmente vicina a $1$,
  allora possiamo costruire un algoritmo che calcola il logaritmo discreto con probabilità 
  almeno $\frac{1}{2}$, quindi se reiteriamo in media due volte l'algoritmo otteniamo la 
  risposta corretta. Questo perché possiamo verificare che il risultato sia corretto.
\end{tcolorbox}
Riusciamo ora a mostrare che se ci viene dato un algoritmo che funziona con vantaggio
polinomiale rispetto a $\frac{1}{2}$, allora possiamo costruire un algoritmo che funziona
con probabilità esponenzialmente vicina a $1$? Assolutamente si, per farlo dobbiamo costruire 
tante istanze indipendenti dello stesso problema e prendendo il risultato osservato la 
maggior parte delle volte.
Come costruiamo gli esperimenti indipendenti, tali per cui nel momento in cui conosciamo la 
risposta al problema costruito, allora troviamo la risposta al problema originale?
\subsection{Costruzione dell'esperimento indipendente $y'$ tale per cui dalla risposta 
$y'$ otteniamo la risposta a $y$}
\begin{theorem}
Sia $y$ un quadrato in $\ZZ_p^*$ e sia $r \in_R \left[ 0,\dots \frac{p-1}{2}\right]$ (\textit{esponente a 
caso per una possibile radice quadrata di un numero, presente nella prima metà}). Sia $y' = y \cdot g^{2x}$, 
dove $y'$ sarà un'altra istanza indipendente del problema dei 
residui quadratici, distribuita uniformemente in $\ZZ_p^*$, di cui conosciamo il logaritmo 
discreto. 
Se $2x + 2r < p - 1$ allora $z \cdot g^r$ è \texttt{PSQR} di $y \cdot g^{2r} 
\iff z$ è \texttt{PSQR} di $y$.
\end{theorem}
\[
    \textit{Sappiamo che } \sqrt{g^{2x} g^{2r}} = 
    \begin{cases}
      g^{x} g^{r}\\
      g^{x} g^{r} g^{\frac{p - 1}{2}}
  \end{cases}
\]
Con l'ipotesi \textcolor{red}{$2x + 2r < p - 1$} allora $g^{x+r}$ è la 
radice quadrata principale.

Questo perché moltiplicando $y \cdot g^{2r}$ abbiamo un'operazione \textbf{aritmetica} poiché 
gli esponenti sono sommati e non si ricade nella 
radice quadrata successiva, ovvero quella dopo la prima metà (\textit{l'esponente di 
$y$ è $2x$}). Moltiplicare quindi fa si che non si vada 
oltre $p - 1$, quindi il logaritmo discreto del prodotto è la somma dei logaritmi discreti;
andar oltre $p - 1$ significa tornare indietro, siccome siamo in un gruppo ciclico.

Dalla risposta al problema trasformato, sappiamo quindi calcolare la risposta al problema originale.

Ma come faccio a far si che $r$ scelta casualmente soddisfi le proprietà del teorema?
\begin{tcolorbox}
  Più $x$ è piccolo e più è probabile trovare un $r$ che soddisfa la proprietà del teorema.
\end{tcolorbox}
\begin{figure}[H]
  \centering 
  \begin{tikzpicture}
    % disegno un segmento orizzontale che va da 0 a (p-1)/2
    \draw[-] (0,0) -- (10,0);
    
    % segni sulla linea
    \foreach \x in {0,1,2,...,10} {
      \draw (\x,0.1) -- (\x,-0.1);
      
    }
    % etichette per gli estremi
    \node[above] at (0.5,0) {$\frac{x}{2}$};
    \node[below] at (0,-0.1) {0};
    \node[below] at (10,-0.1) {\(\frac{p-1}{2}\)};

    % parentesi graffa che racchiude i segmenti
    \draw [decorate,decoration={brace,amplitude=5pt, mirror},xshift=0pt,yshift=-24pt]
    (0,0) -- (10,0) node[midway,below=4pt] {$t$};
  \end{tikzpicture}
\end{figure}
Supponiamo che $\frac{x}{2}$ sia nel primo intervallo, la probabilità di trovare un $r$ dove 
$\frac{x}{2} + r$ non vada oltre $\frac{p - 1}{2}$, ovvero $\frac{t - 1}{t}$.
La probabilità che la risposta sia corretta è la combinazione della probabilità di 
soddisfacibilità del teorema, ovvero $\left( \frac{t - 1}{t}\right)$ e la probabilità
della risposta al problema trasformato, ovvero $\left( \frac{1}{2} + k^{-c}\right)$.

Questo perché qualche volta, dalla risposta al problema trasformato, non si riesce a
trovare la risposta al problema originale.

\begin{equation}
  \probP[\texttt{Esperimento corretto}] = \left( \frac{t - 1}{t}\right) \left( \frac{1}{2} + k^{-c}\right)
  \geq \frac{1}{2} + k^{-2c}
\end{equation}
Risolvendo la disequazione in $t$ riusciamo a capire qual è il limite inferiore a gli intervalli da utilizzare.

Dalla teoria dell'algebra sappiamo che si tratta di una disequazione polinomiale in $t$, come tale 
ha una soluzione polinomiale nelle costanti interne. La soluzione è quindi polinomiale nelle costanti 
presenti nell'equazione. Il limite inferiore è quindi polinomiale nelle costanti presenti nell'equazione.
\begin{equation}
  \frac{t - 1}{t} \geq \frac{1}{2} + k^{-c} \implies t \geq \frac{2}{1 + 2k^{-c}}
\end{equation}
Dividendo in una quantità di intervalli che è polinomiale in $k$ otteniamo un sistema tale per cui, se 
$x$ è nel primo intervallo, la probabilità di successo, ovvero di fornire una radice quadrata principale
è polinomialmente distante da $\frac{1}{2}$, di conseguenza abbiamo un algoritmo che risolve il problema
in tempo polinomiale.

Abbiamo quindi costruito un algoritmo che con probabilità $\frac{1}{2}$ calcola il logaritmo discreto
di un numero, a patto che la metà di tale logaritmo stia nel primo intervallo.
\subsubsection{Generalizzazione dell'intervallo}
Supponiamo di sapere che $\frac{x}{2}$ sia nell'intervallo $i-esimo$.
\begin{figure}[H]
  \centering 
  \begin{tikzpicture}
    % disegno un segmento orizzontale che va da 0 a (p-1)/2
    \draw[-] (0,0) -- (10,0);
    
    % segni sulla linea
    \foreach \x in {0,1,2,...,10} {
      \draw (\x,0.1) -- (\x,-0.1);
      
    }
    % etichette per gli estremi
    \node[above] at (4.5,0) {\textcolor{red}{$\frac{x}{2}$}};
    \node[above] at (3.5,0) {$\dots$};
    \node[above] at (5.5,0) {$\dots$};
    \node[below] at (0,-0.1) {0};
    \node[below] at (10,-0.1) {\(\frac{p-1}{2}\)};

    % parentesi graffa che racchiude i segmenti
    \draw [decorate,decoration={brace,amplitude=5pt, mirror},xshift=0pt,yshift=-24pt]
    (0,0) -- (10,0) node[midway,below=4pt] {$t$};
  \end{tikzpicture}
\end{figure}
Sappiamo che $y = g^x$ e che $i \frac{p - 1}{2t} \leq \frac{x}{2} < (i + 1) \frac{p - 1}{2t}$ 

\[
  g^{x'} = y \cdot g^{-\frac{p - 1}{t}i} \implies 
  g^{x'} = g^x \cdot g^{-\frac{p - 1}{t}i} \implies
  x' = x - \frac{p - 1}{t}i
\]
Quindi $x'$ sta nel primo intervallo, infatti da $\frac{x}{2}$ abbiamo tolto il punto d'inizio 
dell'intervallo $i-esimo$, togliendo tale quantità abbiamo ottenuto il risultato corrispondente al 
primo intervallo.
Ci siamo quindi ricondotti alla risoluzione di un problema relativo al calcolo della radice quadrata 
principale, ovvero il problema che abbiamo già risolto. Dopo aver risolto il problema
ci riconduciamo nuovamente al problema originale moltiplicando per $g^{\frac{p - 1}{t}i}$.

Non conoscendo l'intervallo in cui si trova $\frac{x}{2}$, possiamo provare a risolvere il problema
per tutti gli intervalli. Essendo quantità polinomiali in $k$.

\section{Bit Pseudocasuali}
Vogliamo costruire una sequenza di bit che sia equivalente ad un 
lancio di moneta. Se volessimo utilizzare all'interno di un programma
dei bit che sono casuali, avremmo bisogno di un processo di lancio di moneta 
all'interno del nostro calcolatore. All'interno del processo di lancio 
di moneta utilizzato c'è sempre un punto in cui ci sono scelte casuali da 
dover effettuare e tali scelte possono essere fatte da un processo realmente 
casuale. All'interno di un calcolatore non abbiamo un processo
realmente casuale, poiché i calcoli e le scelte sono deterministiche.
Il processo non è casuale, ma deterministico; ma appare come processo 
casuale agli occhi di chi lo osserva.

L'algoritmo deterministico 
esegue un'operazione sul \textbf{seme} e produce un output.
La generazione non può essere tale che la distribuzione di probabilità degli elementi 
sia uniforme, ma deve essere tale che produca una sequenza di bit tale per cui 
nessuno con potenza di calcolo probabilistica polinomiale sia in grado di 
capirne la distribuzione. Chiunque non si deve accorgere del fatto che 
non stiamo lavorando con sequenze casuali, ma con sequenze pseudocasuali.
\begin{figure}[H]
  \centering
  \begin{tikzpicture}
    \node[draw, rectangle, minimum width=4cm, minimum height=3cm] (A) at (0,0) {$\texttt{PRSG}$};
    \draw[->] (-5, 0) -- (-2, 0) node[midway, above] {$\textit{Seme}$};
    \draw[->] (2, 0) -- (5, 0) node[midway, above] {$b_0,b_1,\dots b_k$};
  \end{tikzpicture}
\end{figure}
Il seme $s$ viene scelto in maniera realmente casuale ed è corto ed ha $k$ bit e
l'output ha $l$ bit, normalmente con $l > k$. Il generatore di numeri pseudocasuali
possiamo vederlo come un \textbf{moltiplicatore di casualità}.
Nessun algoritmo probabilistico polinomiale sarà in grado di trovare 
una regolarità all'interno dell'output generato, nonostante ci sia.
\subsection{Generare bit pseudocasuali}
Avendo a disposizione la sequenza $b_0 b_1 ... b_l$, facciamo fatica ad 
indovinare il bit $b_{l+1}$, lo indoviniamo con un vantaggio $\frac{1}{2} + \epsilon$,
dove $\epsilon$ è più piccolo di qualsiasi polinomio.

\begin{equation}\label{eq:bit-pseudocasuali-uno}
  \bigg| \probP[s \in_R \{0,1\}^k; b_0 b_1 ... b_l \gets \mathcal{G}(s);
  b \gets \mathcal{A}(b_0 b_1 ... b_l); b = b_{l+1}] - \frac{1}{2} \bigg| \leq k^{-\omega(1)}
\end{equation}
La sequenza a disposizione la vedo come sequenza realmente casuale quando 
ogni singolo bit è risultato di un lancio di moneta. 

\begin{tcolorbox}
  Una sequenza di bit è pseudocasuale se nessun algoritmo probabilistico polinomiale
  è in grado di distinguere la sequenza generata da una sequenza realmente casuale.
\end{tcolorbox}

La definizione è un po' strana, perché dice che non siamo capaci di indovinare 
il bit successivo. Ci piacerebbe di più una definizione che dicesse che stiamo 
fornendo bit casuali nel momento in cui dalla sequenza di bit generata non siamo 
capaci di distinguerle da una sequenza realmente casuale.

Per definire un concetto simile dobbiamo essere in grado di dire che non c'è un algoritmo 
che data la sequenza veramente casuale e la sequenza pseudocasuale, non è in grado di
distinguere le due sequenze. Quindi descriviamo la probabilità che l'algoritmo $\mathcal{D}$ dia come risultato 1, 
ovvero che abbia riconosciuto la sequenza pseudocasuale come tale:

\[
  \mathcal{P}_k^{\mathcal{D},\mathcal{G}} = \probP[s \in_R \{0,1\}^k;
  b_0 b_1 ... b_l \gets \mathcal{G}(s); \mathcal{D}(b_0 b_1 ... b_l) = 1]
\]
Descriviamo la probabilità che l'algoritmo $\mathcal{D}$ dia come risultato 1, 
se prende in input una sequenza di bit realmente casuale:
\[
\mathcal{P}_k^{\mathcal{D},\mathcal{R}} = \probP[b_0 b_1 ... b_l \in_R \{0,1\}^{l+1}; \mathcal{D}(b_0 b_1 ... b_l) = 1]
\]
\begin{tcolorbox}
  Quindi la probabilità che il distinguisher restituisca $1$ nei due casi differisce di 
  una quantità più piccola di qualsiasi polinomio.
  
  \begin{equation}\label{eq:bit-pseudocasuali-due}
    \forall_{\mathcal{D} \in \texttt{PPT}} \quad \bigg| \mathcal{P}_k^{\mathcal{D},\mathcal{G}}
    - \mathcal{P}_k^{\mathcal{D},\mathcal{R}} \bigg| < k^{-\omega(1)}
  \end{equation}
\end{tcolorbox}
La formula appena descritta potrebbe essere una nuova definizione di generazione di bit pseudocasuali,
le due equazioni \ref{eq:bit-pseudocasuali-uno} e \ref{eq:bit-pseudocasuali-due} sono quindi equivalenti.
Disporre di definizioni equivalenti è utile perché ci permette di scegliere quella che ci è più comoda 
per il problema che stiamo affrontando.

\subsection{Generazione di bit pseudocasuali basati sul problema del logaritmo discreto} \label{subs:generazione-bit-pseudocasuali}
Fissiamo un numero primo $p$ e un generatore $g$ del gruppo $\mathbb{Z}_p^*$.
Prendiamo il seme $x \in_R \{0,\dots,p-1\}$, e definiamo:
\[
\begin{array}{llllll}
  a_0 & a_1 & a_2 & a_3 & \dots & a_l \\
  x & g^x & g^{g^x} & g^{g^{g^x}} & \dots & g^{g^{g^{\dots^{g^x}}}} \\
  b_0 & b_1 & b_2 & b_3 & \dots & b_l \\
\end{array}
\]
In maniera compatta:
\[
  \begin{cases}
    a_{i + 1} = g^{a_i} \\
    a_0 = x
  \end{cases}
\]
Generiamo elementi di $\mathbb{Z}_p^*$, dove il primo elemento è il seme 
e gli altri elementi sono ottenuti elevando $g$ all'elemento precedente.

A questo punto definisco i bit pseudocasuali come:
\[
  b_i = \begin{cases}
    0 & \text{se } a_i < \frac{p}{2} \\
    1 & \text{se } altrimenti
    \end{cases}
\]
Quindi l'hardcore predicate è $a_i < \frac{p - 1}{2}$
La sequenza pseudocasuale di ritorno sarà:
\[
\begin{array}{llllll}
  b_l & b_{l-1} & b_{l-2} & b_{l-3} & \dots & b_0 \\
\end{array}
\]
Utilizzando la definizione \ref{eq:bit-pseudocasuali-uno}, supponiamo per assurdo che esista 
un algoritmo in grado di predire il bit successivo con un vantaggio più grande 
di qualche polinomio.
I bit presi in input sono $b_l, ..., b_3$, quindi l'algoritmo $\mathcal{A}$ predice il 
bit $b_2$. ma $a_3$ è l'hardcore predicate di $b_2$, quindi non è possibile 
predire il bit $b_2$ con un vantaggio maggiore di qualche polinomio.
\[
  a^3 = g^{g^{g^x}} \quad \text{e} \quad a^2 = g^{g^x}
\]
La costruzione della sequenza 
$a_0, a_1, a_2, a_3, \dots, a_l$ e della sequenza $b_l, b_{l-1}, b_{l-2}, b_{l-3}, \dots, b_0$
è facilmente costruibile. Se i bit fossero stati restituiti in ordine sequenziale, 
$b_0, b_1, b_2, b_3, \dots, b_l$, non saremmo riusciti a far funzionare 
l'algoritmo perché i bit successivi della 
sequenza sarebbero stati il risultato di una funzione facilmente calcolabile.
La nostra costruzione si è basata sul fatto che la funzione da calcolare
sui semi è difficile al fine di dire che non riusciamo a prevedere il bit successivo.

Il fatto che la dimostrazione non funziona non implica che non sia possibile restituire 
i bit in ordine di enumerazione crescente.

La costruzione di funzionamento con i bit restituiti alla rovescia si basa sul fatto che
conoscendo i semi non siamo in grado di calcolare $b_2$, se non so calcolare $b_2$ 
conoscendo i semi, a maggior ragione non sono in grado di calcolarlo conoscendo
i bit, ma tale dimostrazione non mi permette di catturare tale concetto.

Prendendo in considerazione la formula \ref{eq:bit-pseudocasuali-due}, se i bit restituiti
alla rovescia sono indistinguibili da una sequenza casuale, allora anche i bit restituiti
in ordine crescente sono indistinguibili da una sequenza casuale.

\begin{tcolorbox}
  Se $b_l, \dots, b_0$ soddisfa la definizione \ref{eq:bit-pseudocasuali-due}, allora soddisfa 
  la definizione \ref{eq:bit-pseudocasuali-uno}. Quindi
  $b_l, \dots, b_0$ è indistinguibile da una sequenza casuale e possiamo concludere che
  $b_0, \dots, b_l$ è indistinguibile da una sequenza casuale.
\end{tcolorbox}
Sia un distinguisher $\mathcal{D}$ per $b_l, \dots, b_0$, quindi $\mathcal{D}$ prende in input
$b_l, \dots, b_0$ e restituisce $1$ se la sequenza è pseudocasuale e $0$ se la sequenza è casuale.
Sia $\mathcal{D}'$ un distinguisher per $b_l, \dots, b_0$, quindi $\mathcal{D}'$ prende in input
$b_0, \dots, b_l$ e restituisce $\mathcal{D}(b_0, \dots, b_l)$, quindi rovescia la sequenza e 
restituisce il risultato di $\mathcal{D}$ basata sulla sequenza rovesciata.

Se $\mathcal{D}$ distingue $b_l, \dots, b_0$ allora $\mathcal{D}'$ distingue $b_0, \dots, b_l$ 
e viceversa. Quindi distinguere i bit in ordine di enumerazione crescente è equivalente a distinguere i bit
in ordine di enumerazione decrescente.

Restituire i bit alla rovescia è svantaggioso perché non è possibile calcolare il bit successivo, mentre 
restituire i bit in ordine di enumerazione crescente permette di andare avanti con la sequenza.
\subsection{Distinguere equivale a prevedere}
\begin{theorem}
  \begin{enumerate}
    \item Sia distinguere equivalente a:
    \begin{equation}
      \forall_{\mathcal{D} \in \texttt{PPT}} \quad \bigg| \mathcal{P}_k^{\mathcal{D},\mathcal{G}}
      - \mathcal{P}_k^{\mathcal{D},\mathcal{R}} \bigg| < k^{-\omega(1)}
    \end{equation}
    \item Sia prevedere equivalente a:
    \begin{equation}
      \bigg| \probP[s \in_R \{0,1\}^k; b_0 b_1 ... b_l \gets \mathcal{G}(s);
      b \gets \mathcal{A}(b_0 b_1 ... b_l); b = b_{l+1}] - \frac{1}{2} \bigg| \leq k^{-\omega(1)}
    \end{equation}
  \end{enumerate}
  Allora:
  \begin{itemize}
    \item $1 \implies 2$
    \item $2 \implies 1$
  \end{itemize}

\end{theorem}
\begin{proof}
  ($1 \implies 2$) Su input $b_0, \dots, b_{l - 1}$ indoviniamo $b_l$ con vantaggio polinomiale,
  ovvero:
  \[
    \probP[\mathcal{A}(b_0, \dots, b_{l-1}) = b_l] - \frac{1}{2} \geq k^{-c}
  \]
  \[
    \probP[\mathcal{A}(b_0, \dots, b_{l-1}) = b_l] \geq \frac{1}{2} + k^{-c}
  \]
  e possiamo affermarlo senza perdita di generalità. Se vogliamo dimostrare che la proprietà 
  ($2$) vale, ovvero che violando la proprietà ($1$) si viola anche la proprietà ($2$), 
  allora possiamo costruire un distinguisher.
  \[
    \mathcal{D}(b_0, \dots, b_l) = \begin{cases}
      1 & \text{se } \mathcal{A}(b_0, \dots, b_{l-1}) = b_l \\
      0 & \text{altrimenti}
      \end{cases}
  \]
  Sappiamo che la probabilità che l'algoritmo $\mathcal{A}$ indovini 
  un bit scelto in maniera veramente casuale e indipendente da $b_0, \dots, b_{l - 1}$ 
  è esattamente $\frac{1}{2}$ (\textit{one-time pad}), ovvero:
  \[\mathcal{P}_k^{\mathcal{D},\mathcal{U}} = \frac{1}{2}\]
  Mentre, la probabilità che $\mathcal{A}$ con in input $b_0, \dots, b_{l - 1}$ indovini
  $b_l$ è maggiore di $\frac{1}{2} + k^{-c}$, ovvero:
  \[\mathcal{P}_k^{\mathcal{D},\mathcal{G}} \geq \frac{1}{2} + k^{-c}\]

  Di conseguenza:
  \[
    \mathcal{P}_k^{\mathcal{D},\mathcal{G}} - \mathcal{P}_k^{\mathcal{D},\mathcal{U}} \geq \frac{1}{2} + k^{-c} - \frac{1}{2} = k^{-c}
  \]
  Se esiste l'attaccante per la proprietà ($1$), allora violiamo la proprietà ($2$).

  ($2 \implies 1$) Disponiamo di una sequenza di bit $b_1, \dots, b_l$ generata da $\mathcal{G}$ e 
  una sequenza di bit $r_1, \dots, r_l$ generata in maniera casuale e indipendente da $\mathcal{G}$, supponiamo 
  che $\mathcal{D}$ sia un distinguisher per $b_1, \dots, b_l$ e $r_1, \dots, r_l$.
  
  Sappiamo che:
  \begin{align*}
    \begin{array}{r c lllllllll}
    \mathcal{S}_0 & = & b^1 &\dots &b^{i-1} & b^i &b^{i+1} & b^{i+2} &\dots &b^l \\
    \mathcal{S}_{l}& = & r^1 &\dots &r^{i-1} & r^i &r^{i+1} & r^{i+2} &\dots &r^l \\
    \end{array}
  \end{align*}
  Con la tecnica di 
  interpolazione, supponendo che le sequenze differiscano di un solo bit 
  (\textit{come dimostrato nella sezione \ref{sec:distinguisher}}), siamo 
  in grado di affermare che:
  \[
    \bigg| \mathcal{P}_k^{\mathcal{D},\mathcal{G}} - \mathcal{P}_k^{\mathcal{D},\mathcal{R}} \bigg| \geq \frac{1}{2} - k^{-c}
  \]
  \[
    \bigg| \sum_{i = 0}^{l}(\mathcal{P}_i - \mathcal{P}_{i - 1}) \bigg| \leq \sum_{i = 0}^{l} \bigg| (\mathcal{P}_i - \mathcal{P}_{i - 1}) \bigg|
  \]
  Quindi
  \[
    \exists i \in \{0, \dots, l\} \quad \bigg| \mathcal{P}_i - \mathcal{P}_{i - 1} \bigg| \geq \frac{k^{-c}}{l} = k^{-c'}
  \]
  \begin{align*}
    \begin{array}{r c lllllllll}
    \mathcal{S}_i & = & b^1 &\dots &b^{i-1} & b^i &\textcolor{red}{r^{i+1}} & r^{i+2} &\dots &r^l \\
    \mathcal{S}_{i+1}& = & b^1 &\dots &b^{i-1} & b^i &\textcolor{red}{b^{i+1}} & r^{i+2} &\dots &b^l \\
    \end{array}
  \end{align*}
  In qualche modo il distinguisher si comporta diversamente quando i bit differiscono di un solo bit.

  Vorremmo cercare di costruire un algoritmo che indovini i bit pseudocasuali.
  Sia
  \[
    \mathcal{F} = \mathcal{D}(b_0, \dots, b_{i}, r_{i+1}, \dots, r_l)
  \]
  \[
    \mathcal{A}(b_1,...,b_l) = 
    \begin{cases}
      r_{i+1} & \text{se } \mathcal{F} = 1 \\
      \bar{r}_{i+1} & \text{se } \mathcal{F} = 0
    \end{cases}
  \]
  Utilizziamo quindi il distinguisher per indovinare il bit successivo.

  Senza perdita di generalità $\mathcal{P}_{i+1} - \mathcal{P}_i \geq k^{-c'}$,
  supponiamo che la probabilità che $\mathcal{D}$ restituisca $0$ su input $b_1, \dots, b_i, \bar{b}_{i+1}, \dots, r_l$ sia $x$,
  allora la probabilità che $\mathcal{A}$ indovini il bit successivo è:
  \[
    \probP[\mathcal{A}(b_1,...,b_l) = b_{i+1}] = \probP[r_{i+1} = b_{i+1}] \cdot \mathcal{P}_{i+1} + \probP[r_{i+1} = \bar{b}_{i+1}] \cdot x
  \]
  \[
    \frac{1}{2}\cdot \mathcal{P}_{i+1} + \frac{1}{2}\cdot x
  \]
  Proviamo attualmente a cercare una relazione linearmente indipendente con $\mathcal{P}_{i+1}$. Il valore 
  equivale alla probabilità che il distinguisher restituisca $1$ su input $b_1, \dots, b_i, r_{i+1}, \dots, r_l$.
  \begin{align*}
    \mathcal{P}_i &= \probP[\textit{il bit i-esimo} = b_{i+1}] \cdot \probP[\mathcal{D} \textit{ dia 1 su } b_1, \dots, b_i, b_{i+1}, \dots, r_l] +\\
    &+ \probP[\textit{il bit i-esimo} = \bar{b}_{i+1}] \cdot \probP[\mathcal{D} \textit{ dia 0 su } b_1, \dots, b_i, \bar{b}_{i+1}, \dots, r_l] =\\
    &= \frac{1}{2} \cdot \mathcal{P}_{i+1} + \frac{1}{2} \cdot (1 - x) = \frac{1}{2} \cdot \mathcal{P}_{i+1} + \frac{1}{2} - \frac{1}{2} \cdot x
  \end{align*}
  Ricaviamo quindi che:
  \[
    x = \mathcal{P}_{i+1} + 1 - 2 \cdot \mathcal{P}_i
  \]
  Sostituendo $x$ nell'equazione precedente:
  \begin{align*}
    &= \frac{1}{2} \cdot \mathcal{P}_{i+1} + \frac{1}{2} \cdot \mathcal{P}_{i+1} + \frac{1}{2} - \mathcal{P}_i\\
    &= \frac{1}{2} + \mathcal{P}_{i+1} - \mathcal{P}_i  
  \end{align*}
  Ma sappiamo che $\mathcal{P}_{i+1} - \mathcal{P}_i \geq k^{-c'}$, quindi:
  \[
    \frac{1}{2} + \mathcal{P}_{i+1} - \mathcal{P}_i \geq \frac{1}{2} + k^{-c'}
  \]
\end{proof}
\section{Blum Blum Shub}
Si tratta di un algoritmo di generazione di numeri pseudocasuali 
basato sulla difficoltà del calcolo di radici quadratiche in $\mathbb{Z}_n^*$.

La funzione di elevamento al quadrato è una funzione \textit{one-way trapdoor} in $\mathbb{Z}_{p\cdot q}^*$,
poiché la conoscenza di $p$ e $q$, ovvero la fattorizzazione di $n$,
permette di calcolare la radice quadrata in tempo polinomiale.
Sappiamo inoltre che ogni funzione \textit{one-way} ammette un \textit{hardcore predicate}, infatti:
\[
  \texttt{LSB}(x) = (\sqrt{x})
\]
Il problema è che $x$ ha più di una radice quadrata, che radice bisogna selezionare?
\begin{theorem}[Primi di Blum]
  Se $p$ e $q$ sono primi congrui a $3 \mod 4$ allora per ogni quadrato $x$ 
  esiste una sola radice quadrata $y$ tale che $y \equiv \sqrt{x} \mod n$.
\end{theorem}

Sia $n = p \cdot q$ con $p$ e $q$ primi di Blum scelti casualmente, sia 
$x \in_R \mathcal{Z}^*$, allora
\begin{align*}
  \begin{array}{lllll}
    a_1 & a_2 & a_3 & \dots & a_l \\
    x^2 & (x^2)^2 & ((x^2)^2)^2 & \dots & x^(2^l)
  \end{array}
\end{align*}
ovvero
\[
  \begin{cases}
    x_i = x^2\\
    x_{i+1} = a_i^2
  \end{cases}  
\]
Definiamo quindi $b_i = \texttt{LSB}(a_i)$, quindi:
\begin{align*}
  \begin{array}{lllll}
    a_1 & a_2 & a_3 & \dots & a_l \\
    x^2 & (x^2)^2 & ((x^2)^2)^2 & \dots & x^(2^l)\\
    b_1 & b_2 & b_3 & \dots & b_l
  \end{array}
\end{align*}
Dove il valore di ritorno sarà la sequenza $b_l, b_{l-1}, \dots, b_1$.
Utilizzando il principio spiegato nella sezione precedente (\ref{subs:generazione-bit-pseudocasuali}) 
possiamo restituire i bit in ordine di enumerazione crescente, ovvero $b_1, b_2, \dots, b_l$.

La differenza è che se il numero $n$ è conosciuto da tutti, allora è possibile restituire i bit solamente 
nell'ordine $b_1, b_2, \dots, b_l$, altrimenti in qualsiasi ordine.

\section{Crittosistema di Blum Goldwasser}
L'idea è stata di utilizzare un crittosistema dato dai soliti tre algoritmi, quello di generazione 
della chiave, quello di codifica e quello di decodifica.
La chiave è data da $p, q$ primi di Blum con $k$ bit, $n = p \cdot q$, dove la chiave pubblica 
è data da $n$ e la chiave privata è data da $p$ e $q$.
Chi possiede la chiave segreta riesce a calcolare radici quadrate in modulo $n$. L'algoritmo 
di codifica è dato da un $x\in_R \mathbb{Z}_n^*$, e attraverso blum blum shub, generiamo 
$l$ bit pseudocausali $b_1, \dots, b_l$. Supponiamo di avere un messaggio $m$ di lunghezza $l$,
allora il messaggio cifrato sarà:
\[
  c_1, \dots, c_l, x^{2^{l+1}} = m_1 \oplus b_1, \dots, m_l \oplus b_l
\]
Dove $x^{2^{l+1}}$ è il primo elemento della sequenza generata da blum blum shub.

Sostanzialmente il messaggio viene cifrato attraverso one time pad, 
con una chiave non realmente casuale, ma con numeri pseudocasuali mediante un seme.

Se la sequenza $b_1, \dots, b_l$ è casuale, allora il cyphertext non contiene 
alcuna informazione sul plaintext, quindi il crittosistema è sicuro.

Supponendo per assurdo che dal ciphertext sia possibile ricavare informazioni sul plaintext,
allora si riesce a distinguere qualunque coppia di messaggi, ma sappiamo che non lo si riesce a 
fare in presenza di una chiave realmente casuale. Ma nel caso specifico del nostro crittosistema, 
si dovrebbe riuscire a distinguere sul generato di bit pseudocausali, ma questo è assurdo 
poiché non siamo in grado di distinguere una sequenza pseudocasuale da una casuale.

La decodifica ricava $x^2$, infatti, data la conoscenza della fattorizzazione di $n$, 
è possibile calcolare $\sqrt{x^{2^{l+1}}} = x^{2^l}$, e così via. Si ricava quindi
$x^{2^l}, x^{2^{l-1}}, \dots, x^2$ e si calcolano i bit $b_1, \dots, b_l$. Attraverso 
questi bit è possibile decifrare il messaggio con l'operazione inversa:
\[
  m_i = c_i \oplus b_i
\]

L'algoritmo in questione effettua il calcolo di $l$ quadrati,
con un costo di $\Theta(n^2)$ per ciascuna operazione di elevamento
al quadrato. Pertanto, il costo totale dell'algoritmo è $\Theta(l \cdot n^2)$.
Questo algoritmo è uno stream cypher, a differenza di \texttt{RSA}, che è invece
un block cypher. Nonostante questa differenza nel metodo di cifratura, le complessità
computazionali di entrambi gli algoritmi risultano essere simili. La distinzione principale
risiede nel fatto che l'algoritmo di Blum Goldwasser gode di una dimostrazione di sicurezza,
dimostrabilità che manca invece in \texttt{RSA}.