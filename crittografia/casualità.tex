\chapter{Costruzione della casualità}
\section{Distinguisher}\label{sec:distinguisher}
Codificare $b_1, \dots, b_l$ in $E(b_1), \dots, E(b_l)$ può essere fatto facendo si che dal testo cifrato non si risalga 
al testo in chiaro. Dimenticandoci del problema della malleabilità, vorremmo che dal cyphertext non si possa risalire
ad alcuna informazione binaria sul plaintext, ma dobbiamo capire cosa vuol dire non poter risalire ad alcuna informazione
binaria.
\begin{tcolorbox}[title = Distinguisher]
  Un \textbf{distinguisher} è un algoritmo binario $\mathcal{D} \in \texttt{PPT}$ che restituisce $0$ o $1$. Il distinguisher
  prende in input un evento che soddisfa o meno una certa proprietà e 
  un altro evento che soddisfa o meno la stessa proprietà. Il distinguisher deve essere in grado di distinguere
  se i due eventi si comportano in modo diverso o meno.
  Statisticamente il distinguisher verifica che i due eventi si comportino in modo polinomialmente diverso
  (\ref{limite_chernoff}), altrimenti tale differenza non sarebbe distinguibile.
\end{tcolorbox}
Sia $\probP_k^{\mathcal{D}, m}$ la probabilità che il distinguisher $\mathcal{D}$ restituisca $1$ su input, una codifica 
di $m$ quando il security parameter vale $k$. Un sistema di codifica nascone $m_1$ e $m_2$ a $\mathcal{D}$ quando:
\begin{equation}
  \forall c \, \exists \bar{k} \, \forall k \geq \bar{k} \qquad
  |\probP_k^{\mathcal{D}, m_1} - \probP_k^{\mathcal{D}, m_2}| < k^{-c}
\end{equation}
Ovvero:
\begin{equation}
  |\probP_k^{\mathcal{D}, m_1} - \probP_k^{\mathcal{D}, m_2}| < k^{-\omega(1)}
\end{equation}
$E$ nasconde informazioni a $\mathcal{D}$ se per ogni $m_1$ e $m_2$ $E$ nasconde $m_1$ e $m_2$ a $\mathcal{D}$. 
$E$ nasconde se per ogni $\mathcal{D} \in \texttt{PPT}$ $E$ nasconde $m_1$ e $m_2$ a $\mathcal{D}$.
\begin{proof}
  Supponiamo per assurdo che $\exists \mathcal{D} \in \texttt{PPT}$ tale che 
  \[
    \exists m_1, m_2, \,\forall \bar{k} \, \exists k \geq \bar{k} \qquad
    |\probP_k^{\mathcal{D}, m_1} - \probP_k^{\mathcal{D}, m_2}| \geq k^{-c}
  \]
   Allora $\mathcal{D}$ può distinguere su due messaggi $m_1$ e $m_2$ che differiscono di un solo bit.
   \begin{equation*}
    m_1 = \alpha^1 \alpha^2 \dots \alpha^l \qquad
    m_2 = \beta^1 \beta^2 \dots \beta^l 
   \end{equation*}

Definisco $\forall i \in \{0,\dots,l\} \,m(i) = \beta^1 \dots \beta^{i-1} \beta^i \alpha^{i+1} \dots \alpha^l$, 
ovvero una serie di bit intermedi per poter passare da $m_1$ a $m_2$ variando un solo bit per volta.
Quindi $m(0) = m_1$, $m(l) = m_2$, di conseguenza $\probP_k^{\mathcal{D}, m(i)} = \probP(i)$, ovvero la probabilità 
che il distinguisher restituisca $1$ se viene data in input una codifica del messaggio $m_i$.
  \[
    \probP(0) = \probP_k^{\mathcal{D}, m_1} \qquad \probP(l) = \probP_k^{\mathcal{D}, m_2}
  \]
Scopriamo quindi che l'ipotesi diventa:
\[
  k^{-c} \leq |\probP(0) - \probP(l)| = |\sum_{i=0}^{l - 1} \probP(i) - \probP(i+1)| 
  \leq \sum_{i=0}^{l - 1} |\probP(i) - \probP(i+1)|
\]
Poiché sappiamo che $\probP(0) - \probP(1) + \probP(1) - \probP(2) + \dots + \probP(l-1) - \probP(l) = \probP(0) - \probP(l)$. 
Quindi:
\[
  \sum_{i=0}^{l - 1} |\probP(i) - \probP(i+1)| \geq k^{-c}
\]
Avendo la somma di numeri che eccede un determinato valore allora so che esiste almeno un elemento che è maggiore o uguale 
della media.
\[
  \exists i \in \{0,\dots,l-1\}\qquad \textit{t.c.} \qquad |\probP(i) - \probP(i+1)| \geq \frac{k^{-c}}{l} \geq k^{-(c+1)} = k^{-c'}
\]
Abbiamo quindi trovato un polinomio $c'$ tale per cui il distinguisher è in grado di distinguere due messaggi che differiscono
di un solo bit.
I due messaggi $m_1$ e $m_2$ differiscono di un solo bit, quindi:
\begin{align*}
  \begin{array}{r c lllllllll}
  m(i) & = & \beta^1 &\dots &\beta^{i-1} & \beta^i &\textcolor{red}{\alpha^{i+1}} & \alpha^{i+2} &\dots &\alpha^l \\
  m(i+1) & = & \beta^1 &\dots &\beta^{i-1} & \beta^i &\textcolor{red}{\beta^{i+1}} & \alpha^{i+2} &\dots &\alpha^l \\
  \end{array}
\end{align*}
A questo punto supponiamo senza perdita di generalità che $\alpha^{i+1} = 0$ e $\beta^{i+1} = 1$. Sia $z$ un 
elemento di $\mathbb{Z}_n^*$ con $\left( \frac{z}{n} \right) = 1$, quindi $z$ potrebbe essere la codifica di uno 
$0$ o di un $1$.

Dato $z$ viene costruito $E(\beta_1)E(\beta_2)\dots E(\beta_i)zE(\alpha_{i+2})\dots E(\alpha_l)$ e viene lanciato 
l'algoritmo $\mathcal{D}$ sul risultato. La probabilità con cui $\mathcal{D}$ restituisce $1$ è \probP(i) se $z$
codifica $0$ e $\probP(i+1)$ se $z$ codifica $1$ se in input viene data la codifica di $m(i)$ secondo 
l'algoritmo per codificare il messaggio $m_i$, ovvero applicando $E$ a tutti i bit del messaggio.
Avendo però aggiunto $z$ in mezzo al messaggio non ho codificato secondo l'algoritmo che il distinguisher
si aspetta, quindi bisogna codificare il messaggio $m(i)$ in maniera corretta:
\[
  E(\beta_1)E(\beta_2)\dots E(\beta_i)\,\textcolor{red}{z \cdot x^2}\,E(\alpha_{i+2})\dots E(\alpha_l)\qquad \text{con } 
  x \in \mathbb{Z}_n^*
\]
A questo punto il distinguisher restituisce $1$ con probabilità $\probP(i)$ se $z$
codifica $0$ e $\probP(i+1)$ se $z$.

Chiamiamo $\mathcal{P}_0$ la probabilità che il distinguisher restituisca $0$ (\textit{quindi $\probP(i)$}) e 
$\mathcal{P}_1$ la probabilità che il distinguisher restituisca $1$ (\textit{ovvero $\probP(i+1)$}) se in input dato è costruito come sopra.

Se si riceve la codifica di un bit scelto a caso, con quale probabilità si riesce a distinguere se è $0$ o $1$?
Si utilizza il risultato del distinguisher $\mathcal{D}$ come tentativo.
\[
  \probP[\texttt{indovinare}] = \frac{1}{2} \cdot (1 - \mathcal{P}_0) + \frac{1}{2} \cdot \mathcal{P}_1 = 
  \frac{1}{2} + \frac{1}{2} \cdot (\mathcal{P}_1 - \mathcal{P}_0)
\]
Se sottraiamo $\frac{1}{2}$ otteniamo:
  \begin{align*}
    \probP[\texttt{non indovinare}] &= \frac{1}{2} + \frac{1}{2} \cdot (\mathcal{P}_1 -
  \mathcal{P}_0) - \frac{1}{2} \\
  &= \frac{1}{2} \cdot (\mathcal{P}_1 - \mathcal{P}_0) \\
  &= \frac{1}{2} \cdot |\probP(i) - \probP(i+1)| \\
  &\geq \frac{1}{2} \cdot k^{-c'}
  \end{align*}
L'algoritmo ha quindi un vantaggio di almeno $\frac{1}{2} \cdot k^{-c'}$, quindi tale algoritmo è 
un attaccante.
\end{proof}
%\subsection{Distinguere e indovinare}
%TODO
\section{Lancio della moneta}
Il sistema descritto in precedenza ha un problema, infatti sappiamo che 
la codifica di un singolo bit richiede una quantità molto alta di bit. Infatti 
implicherebbe un enorme spreco di banda, di risorse. Ci piacerebbe giungere 
ad uno scenario in cui un singolo bit viene codificato con un solo bit.

Per arrivare a questo punto impareremo a costruire numeri \textbf{pseudo-casuali
crittograficamente sicuri} da utilizzare come base per simulare un one-time pad, 
dove la chiave non è realmente casuale, ma è una chiave generata da noi, 
ma nessuno con potenza di calcolo polinomiale possa realmente notare la 
differenza.

Il lancio di monete in rete è un problema molto importante, infatti simulare 
un reale lancio di monete è molto difficile, in quanto non si può avere
la certezza che uno dei due agenti non stia mentendo.
Disponiamo dei due agenti $A$ e $B$ che vogliono simulare un
lancio di moneta e che comunicano attraverso un canale di comunicazione
e vorremmo che il lancio avvenga in modo simile a quello reale, ovvero
che entrambi gli agenti non sappiano il risultato del lancio prima che
questo avvenga e che il risultato sia frutto del lancio di una moneta.

Il fatto che $A$ lanci la moneta e che comunichi il risultato a $B$ non
va bene, poiché il risultato può essere manipolato da $A$ affinché sia 
di suo gradimento, e l'altro agente non può avere la certezza che il
risultato sia reale.
Si potrebbe provare a comunicare il risultato in contemporanea, ma anche
in questo caso si potrebbe simulare un ritardo di rete in modo da 
sfruttare il risultato a proprio vantaggio.
Ci si potrebbe affidare ad un terzo agente, ma anche in questo caso
l'agente esterno deve essere fidato.

Abbiamo quindi dato due risultati al lancio della moneta, ovvero
$Coin_a$ e $Coin_b$. $Coin_a$ è una variabile del processo locale dell'agente
$A$ e $Coin_b$ è una variabile del processo locale dell'agente $B$, ed entrambe
conterranno il risultato del lancio della moneta. Se $A$ e $B$ seguono 
il protocollo (\textit{sono onesti}), allora $Coin_a = Coin_b$, con 
\[
  \probP[Coin_a = 0] = \frac{1}{2}
\]
Potrebbero però non seguire il protocollo e indurre l'altro agente in errore, ottenendo 
un risultato iniquo, nel nostro caso $A$ segue il protocollo, mentre $B$ no, non possiamo 
fornire alcuna garanzia su $B$, possiamo però tutelare $A$. Per tutelarlo, 
$A$ deve essere in grado di osservare il risultato di un lancio di una 
moneta.
\[
  | \probP[Coin_a = 0] - \frac{1}{2} | < k^{-\omega(1)}
\]
Ammettiamo che $B$ possa alterare tale probabilità, ma non la può alterare di troppo,
quindi la differenza tra la probabilità reale e quella che $B$ può alterare deve essere
minore di qualsiasi polinomio. Per ogni esponente esiste un valore minimo della 
lunghezza della chiave tale per cui con chiavi sufficientemente lunghe, 
la probabilità si discosta da $\frac{1}{2}$ di un valore minore di $k^{-\omega(1)}$.
\subsection{Lancio della moneta con il residuo quadratico}
Ci basiamo sulla difficoltà di stabilire se un numero è un quadrato o meno in $\ZZ_p^*$.
\pseudocodeblock{
\begin{tikzpicture}
  \node[businessman, monitor, female, minimum size=2cm,label=below:{Alice}] (alice) {};
\end{tikzpicture}
\<\<
\begin{tikzpicture}
  \node[dave, monitor, mirrored, minimum size=2cm, label=below:{Bob}] (bob) {};
\end{tikzpicture}
\\[0.1\baselineskip][\hline] \\[-0.5\baselineskip]
\text{Sceglie }p,q \in_R primi \\
\text{Calcola } n=p\cdot q \\
\text{Calcola } z \in \ZZ_n^* \text{ con } \left(\frac{z}{n}\right)=1\\
\< \sendmessageright*{n,z} \\
\<\< \text{Sceglie } b \in_R \{0,1\} \\
\< \sendmessageleft*{b} \\
\< \sendmessageright*{p, q} \\
}

Il risultato finale sarà $b \oplus \texttt{isSquare}(z)$. Sostanzialmente 
$A$ lancia una moneta (\textit{sceglie a caso fra un quadrato o un non quadrato, poiché 
scegliendo uniformemente fra gli elementi di $\ZZ_n^*$ con simbolo 
di Jacobi $1$, si ottiene un quadrato con probabilità $\frac{1}{2}$}) e
$B$ sceglie un bit $b$ a caso e lo invia ad $A$. Quando $B$ riceve $z$ non 
sa qual è il risultato del lancio della moneta, perché non sa risolvere il 
problema del residuo quadratico, di conseguenza è vero che $A$ invia prima 
il risultato del proprio lancio della moneta, ma nella condizione in cui $B$ non 
vede il risultato, ma $B$ non invia il bit $b$ in funzione di $z$.

Se entrambi gli agenti sono onesti, $z$ è stato campionato secondo 
una misura causale, quindi $z$ è un quadrato con probabilità $\frac{1}{2}$ 
e un non quadrato con probabilità $\frac{1}{2}$. Anche $b$ è stato
campionato secondo una misura causale, quindi $b$ è $0$ con probabilità
$\frac{1}{2}$ e $1$ con probabilità $\frac{1}{2}$. Alla fine del protocollo,
tutti sono in grado di calcolare il risultato perché tutti conoscono 
gli interi parametri dell'algoritmo, che sarà uguale per entrambi seguendo la
formula: $b \oplus \texttt{isSquare}(z)$.

Se $B$ è disonesto, allora può calcolare il bit $b$ non casuale, distribuendolo
in maniera differente. Se $B$ calcola $b$ \textbf{indipendentemente} da 
$z$, allora qualunque sia la regola di calcolo di $b$, sarà sempre 
uguale alla quadraticità di $z$ con probabilità $\frac{1}{2}$. Un altro modo 
per imbrogliare è quello di calcolare $b$ in funzione di $z$, ma 
questo richiede la conoscenza dell'algoritmo della quadraticità di $z$.
$B$ ha il vantaggio di allontanarsi da $\frac{1}{2}$, ma è il vantaggio 
pari a quello della risoluzione del problema del residuo quadratico, quindi 
più piccolo di qualsiasi polinomio.

Se $A$ è disonesto, può calcolare $p$ e $q$ non primi, ma $B$, alla fine del 
protocollo se ne accorgerebbe ricevendo $p$ e $q$ e quindi potrebbe
lanciare una propria moneta per decidere il proprio risultato o, se in conoscenza 
del risultato che gli è favorevole, può decidere come risultato un valore che gli 
fornirebbe tale vantaggio. 
$A$ potrebbe non inviare $p$ e $q$, in questo caso $B$ ha un enorme 
svantaggio, perché dopo il secondo messaggio $A$ conosce $Coin_A$, ma 
$B$ non conosce $Coin_B$, se $A$ invia il proprio messaggio a $B$, $B$ 
è in grado di calcolare $Coin_B$ e quindi il risultato finale, ma se $A$
sa il risultato vincente e il risultato ottenuto è sfavorevole, $A$ non 
spedisce il terzo messaggio e $B$ lancia la propria moneta che con probabilità 
$\frac{1}{2}$ è favorevole, ma con probabilità $\frac{1}{2}$ è sfavorevole,
ma si crea una situazione sfavorevole a $B$
\[
  \probP[Coin_B = 0] = \frac{1}{2} + \frac{1}{2} \cdot \frac{1}{2} = \frac{3}{4}
\]
Il comportamento di $A$ ha alterato la misura di probabilità di successo di $B$, a 
favore di $A$.

Il protocollo appena descritto non soddisfa le proprietà di un protocollo di lancio della 
moneta. Ci sono casi in cui il protocollo però può essere seguito, ovvero quando chi 
impersona $A$ non conosce il risultato vincente e $B$ può o non può conoscere il risultato 
vincente.
\begin{tcolorbox}
  Un protocollo per funzionare deve essere tale che gli agenti possano conoscere il 
  risultato anche senza l'ultimo messaggio. Se l'ultimo messaggio è quello significativo,
  allora il protocollo non è corretto.
\end{tcolorbox}
Sia $n$ il numero minimo di messaggi di un protocollo che funziona, quindi gli agenti conoscono 
il risultato del lancio della moneta anche senza l'invio dell'ultimo messaggio.
ciò vuol dire che l'ultimo messaggio non serve, ma se non serve allora il penultimo 
messaggio è quello che permette ad $A$ di conoscere il risultato, ma se $B$ decide di non 
inviare tale messaggio si innesca una reazione a catena che porta $A$ e $B$ a non inviare nulla.
\begin{tcolorbox}
Non esiste il protocollo per il lancio della moneta che soddisfi le condizioni dettate da noi.
Il protocollo funziona solo se, con $n$ agenti, ad imbrogliare sono meno di $\frac{n}{2}$ agenti.
\end{tcolorbox}
Indeboliamo la soluzione in modo che funzioni.
\subsubsection{Lancio di moneta nel pozzo}
L'idea è quella di utilizzare un pozzo, in cui finirà la moneta. $A$ riesce a vedere il risultato
del lancio della moneta, ma $B$ no, perché si trova di intralcio un cane tenuto 
al guinzaglio da $A$. $A$ può decidere far si che $B$ possa vedere il risultato, facendo rientrare 
il cane che si trova nei pressi del pozzo, ma $A$ può anche decidere di non far rientrare il cane.

$A$ non può variare il risultato del lancio della moneta, ha solamente la facoltà di impedire o meno 
a $B$ di vedere il risultato. Sa a $B$ decidere se partecipare al processo o meno, non gli interessa
il risultato del lancio della moneta, ma interessa solamente che la probabilità del lancio sia di 
almeno $\frac{1}{2}$.
\subsubsection{Oblivious transfer}
Supponiamo che $B$ sappiamo il risultato della borsa del prossimo anno e che chieda a ad $A$ di 
avere in cambio del denaro per i risultati della borsa. $B$ non vuole dare tutto il denaro ad $A$,
ma vuole dare solamente una parte del denaro. A questo punto $A$ decide di far il lancio della moneta
per decidere se mostrare o meno il risultato della borsa a $B$. $B$ a questo punto deve trasferire 
l'informazione ad $A$, ma \textbf{non gli interessa se l'informazione è stata trasferita o meno}, ma solamente
che la probabilità che l'informazione sia stata trasferita sia di almeno $\frac{1}{2}$.
\pseudocodeblock{
\begin{tikzpicture}
  \node[businessman, monitor, female, minimum size=2cm,label=below:{Alice}] (alice) {};
\end{tikzpicture}
\<\<
\begin{tikzpicture}
  \node[dave, monitor, mirrored, minimum size=2cm, label=below:{Bob}] (bob) {};
\end{tikzpicture}
\\[0.1\baselineskip][\hline] \\[-0.5\baselineskip]
\<\<\text{Sceglie }p,q \in_R primi \\
\<\<\text{Calcola } n=p\cdot q \\
\<\<\text{Calcola } e, d \text{ chiave } p \text{ e } p  \text{ per } \texttt{RSA} \\
\<\<\text{Codifica il segreto con \texttt{RSA} ottenendo} \\
\<\<\text{un messaggio } c \\
\<\sendmessageleft*{n,e,c} \\
\text{Sceglie } x \in_R \ZZ_n^* \\
\<\sendmessageright*{x^2} \\
\<\<\text{Calcola una radice quadrata di } x, \text{ che sarà } y \\
\<\sendmessageleft*{y} \\
}
Quindi $A$ invia un quadrato a caso e $B$ invia una delle quattro radici quadrate di $x$.
Con probabilità $\frac{1}{2}$ tale radice sarà diversa da $\pm x$. Quando un agente possiede 
due radici quadrate di uno stesso numero \textbf{che non sono una l'opposta dell'altro}, allora 
è in grado di fattorizzare $n$.
Con probabilità $\frac{1}{2}$, $B$ invierà ad $A$ una radice diversa da $\pm x$, metterà quindi 
$A$ nelle condizioni di calcolare $p$ e $q$ e quindi $d$ per decodificare il messaggio $c$.
$B$ non ha alcun interesse nel risultato, che otterrà con probabilità $\frac{1}{2}$, ma
interessa che $A$ abbia sia riuscita a decifrare $c$ con probabilità $\frac{1}{2}$.

Il vantaggio di $A$ è implicito nel risultato.
\subsection{Lancio della moneta con il logaritmo discreto}
Il problema è che fino ad ora abbiamo lavorato con la quadraticità di un numero 
in $\ZZ_n^*$, ma vorremmo lavorare con singoli bit. Quindi non con l'inversa di una 
funzione one way, ma con singoli bit, ovvero con l'informazione \textbf{binaria} 
dell'inversa di una funzione one way.

Disponiamo di un numero $p$ e un generatore $g$ di $\ZZ_p^*$, quindi $g$ è un numero
che genera tutti gli elementi di $\ZZ_p^*$, ovvero $\{g^0, g^1, \ldots, g^{p-1}\}$.
I due valori $p$ e $g$ sono condivisi tra $A$ e $B$, prima dell'inizio del protocollo.
\pseudocodeblock{
\begin{tikzpicture}
  \node[businessman, monitor, female, minimum size=2cm,label=below:{Alice}] (alice) {};
\end{tikzpicture}
\<\<
\begin{tikzpicture}
  \node[dave, monitor, mirrored, minimum size=2cm, label=below:{Bob}] (bob) {};
\end{tikzpicture}
\\[0.1\baselineskip][\hline] \\[-0.5\baselineskip]
\text{Sceglie }x \in_R \{0,\dots,p-2\} \\
\text{Calcola } z = g^x \\
\<\sendmessageright*{z} \\
\<\<\text{Sceglie } b \in_R \{0,1\} \\
\<\sendmessageleft*{b} \\
\<\sendmessageright*{x} \\
}
Il risultato è quindi $b \oplus \left( x < \frac{p-1}{2}\right)$. L'idea di fondo è che $B$ 
quando sceglie $b$ non è in grado di dedurre il risultato del lancio della moneta, perché
dovrebbe verificare se $x$ è minore o meno di $\frac{p-1}{2}$, facendo quindi un test binario 
su il \textbf{logaritmo discreto} di $z$. Nel momento in cui $A$ invia $x$, $B$ è in grado
di verificare la correttezza del risultato e a calcolare il risultato.

\begin{tcolorbox}
  $B$ non è in grado di calcolare il predicato binario $x < \frac{p-1}{2}$, a meno di un 
  vantaggio più piccolo di qualsiasi polinomio.
\end{tcolorbox}
Tale algoritmo utilizza un \textbf{predicato binario sull'inversa di una funzione one way}.
Data una funzione one way $f$, un predicato binario $P$ sull'inversa di $f$, non siamo 
certi che il predicato sia difficile da calcolare. 

Per il logaritmo discreto, il predicato binario facile da calcolare è il \textbf{bit 
meno significativo} di x. Tale bit vale zero se e solo se $z$ è un quadrato in $\ZZ_p^*$.
Ma tale predicato dice qualcosa di differente, ovvero ci chiediamo se il logaritmo discreto di 
$z$ sta nella prima metà o nella seconda metà dei logaritmi discreti di $\ZZ_p^*$ o nella seconda,
ovvero una sorta di \textbf{bit più significativo} di $x$.
\section{Hardcore predicate del logaritmo discreto}
\begin{tcolorbox}[title = Hardcore predicate]
  Si tratta di un predicato binario sull'inversa di una funzione one way, che è difficile da
  calcolare. Se scegliessimo a caso il valore di tale predicato binario, la 
  probabilità con cui un algoritmo riesce a calcolare il predicato conoscendo 
  $z$ è $\frac{1}{2} + \epsilon$, con $\epsilon$ più piccolo di qualsiasi 
  polinomio.
\end{tcolorbox}
Sia $y = g^x$ e sia il predicato in considerazione $x < \frac{p - 1}{2}$. Se 
  $y$ è un quadrato, allora ammette due radici quadrate, $z_1$ e $z_2$,
  di queste due radici, una ha logaritmo discreto minore di $\frac{p-1}{2}$ e l'altra
  maggiore o uguale a $\frac{p-1}{2}$. Questo perché le rispettive radici quadrate
  sono nella forma $g^{i}$ e $g^{i + \frac{p-1}{2}}$. La radice quadrata 
  nella forma $g^{i}$ è chiamata \textbf{radice quadrata principale}.

  Di base se abbiamo due radici quadrate di $y$, che sappiamo 
  calcolare aritmeticamente, non sappiamo come capire quale sia
  la radice quadrata principale, ma se 
  avessimo a disposizione l'algoritmo per calcolare se il logaritmo 
  discreto di un numero è minore di $\frac{p-1}{2}$, allora potremmo
  calcolare la radice quadrata principale di un quadrato.

  Supponiamo di avere un algoritmo che date le due radici quadrate di un numero 
  distingue la radice quadrata principale. In questo caso possiamo costruire
  un algoritmo che calcola il predicato binario $x < \frac{p-1}{2}$.
  \begin{theorem}
    Qualunque funzione one-way ammette un hardcore predicate.
  \end{theorem}
  Ne deriva quindi che:
  \begin{theorem}
  Se esiste un algoritmo per il calcolo della radice principale 
  di un quadrato in $\ZZ_p^*$, allora esiste un algoritmo efficiente
  per il calcolo del logaritmo discreto in $\ZZ_p^*$.
\end{theorem}
\begin{theorem}
  Se esiste che riesce  a calcolare il predicato binario con vantaggio 
  polinomiale rispetto al caso casuale, allora esiste un algoritmo
  probabilistico polinomiale per il calcolo del logaritmo discreto.
\end{theorem}
Per dimostrare il teorema dovremmo seguire tre passaggi:
\begin{enumerate}
  \item Mostrare che siamo in grado di calcolare il logaritmo discreto
  di un numero avendo in mano un algoritmo per il calcolo della radice
  quadrata principale di un quadrato.
  \item Mostrare che tale algoritmo funziona anche se la 
  radice quadrata principale è calcolata con probabilità 
  esponenzialmente vicina a $1$.
  \item Mostrare che partendo da un algoritmo che funziona con vantaggio 
  polinomiale rispetto al caso casuale, siamo in grado di costruire un
  algoritmo che funziona con probabilità esponenzialmente vicina a $1$.
\end{enumerate}
\subsection{Calcolare il logaritmo discreto avendo a disposizione
l'algoritmo per il calcolo della radice quadrata principale (\texttt{PSQR})}
Avremo a disposizione l'algoritmo \texttt{PSQR} che calcolerà la radice
quadrata principale di un quadrato in $\ZZ_p^*$, avendo a disposizione 
il predicato binario. Per farlo prende in input $y$ ne calcola la radice 
quadrata, verifica il predicato binario per capire se è la radice quadrata
principale, se la verifica fallisce, calcola la radice principale 
opposta e restituisce il risultato.

\begin{algorithmic}[1]
  \Procedure{LSB}{$y$}
    \If{$\frac{y}{p} = 0$}
      \State \Return $0$
    \Else
      \State \Return $1$
    \EndIf
  \EndProcedure
\end{algorithmic}

Ricordiamo che la procedura $\texttt{LSB}$ verifica se il bit meno significativo
di un numero è pari o dispari. Se è pari restituisce $0$, altrimenti
restituisce $1$.

\begin{algorithmic}[1]
  \Procedure{DiscreteLogarithm}{$y$}
    \If{$y = 1$} 
      \State \Return $0$ \EndIf
    \State $b \gets \texttt{LSB}(DiscreteLogarithm(y))$
    \If{$b = 1$}
      \State \Return $y  \gets y \cdot g^{-1}$ \Comment{imposta a zero il bit meno significativo}
    \EndIf
    \State $y \gets \texttt{PSQR}(y)$ \Comment{scorri a destra di un bit}
    \State \Return $2 \cdot \texttt{DiscreteLogarithm}(y) + b$
  \EndProcedure
\end{algorithmic}
Per calcolare la radice quadrata abbiamo necessariamente bisogno del bit meno significativo
a zero. 

L'algoritmo ricorsivo permette di calcolare il logaritmo discreto di un numero calcolando 
il bit meno significativo e riconducendo il calcolo dello stesso problema 
in una situazione in cui si ha un bit in meno.

\begin{tcolorbox}
  Se abbiamo un algoritmo che funziona per il calcolo della radice quadrata principale
  allora possiamo costruire un algoritmo che calcola il logaritmo discreto.
\end{tcolorbox}

Supponiamo di avere un algoritmo $B$ per \texttt{PSQR} che funziona con probabilità 
esponenzialmente vicina a $1$, ovvero $1 - \epsilon$. La probabilità che l'algoritmo
\texttt{DiscreteLogarithm} invocato $k$ volte fornisca sempre la risposta corretta
è $(1 - \epsilon)^{k}$. L'algoritmo \texttt{DiscreteLogarithm} può non funzionare, 
ma abbiamo modo di accorgerci:
\begin{itemize}
  \item Se il numero di passi è maggiore del numero di bit di $y$, allora
  siamo sicuri che il \texttt{PSQR} non ha funzionato.
  \item Sul risultato finale possiamo verificare che $g^x = y$. Se non è così
  allora \texttt{PSQR} non ha funzionato.
\end{itemize}
In media se funziona con probabilità $(1 - \epsilon)^k$, allora il numero di volte
che funziona in cui si dovrà lanciare l'algoritmo sarà 
$\frac{1}{(1 - \epsilon)^k}$.
Se $\epsilon \leq \frac{1}{2}$, allora $(1 - \epsilon)^k \geq \frac{1}{2^k}$.
\begin{tcolorbox}
  Se abbiamo un algoritmo che funziona con probabilità esponenzialmente vicina a $1$,
  allora possiamo costruire un algoritmo che calcola il logaritmo discreto con probabilità 
  almeno $\frac{1}{2}$, quindi se reiteriamo in media due volte l'algoritmo otteniamo la 
  risposta corretta. Questo perché possiamo verificare che il risultato sia corretto.
\end{tcolorbox}
Riusciamo ora a mostrare che se ci viene dato un algoritmo che funziona con vantaggio
polinomiale rispetto a $\frac{1}{2}$, allora possiamo costruire un algoritmo che funziona
con probabilità esponenzialmente vicina a $1$? Assolutamente si, per farlo dobbiamo costruire 
tante istanze indipendenti dello stesso problema e prendendo il risultato osservato la 
maggior parte delle volte.
Come costruiamo gli esperimenti indipendenti, tali per cui nel momento in cui conosciamo la 
risposta al problema costruito, allora troviamo la risposta al problema originale?
\subsection{Costruzione dell'esperimento indipendente $y'$ tale per cui dalla risposta 
$y'$ otteniamo la risposta a $y$}
\begin{theorem}
Sia $y$ un quadrato in $\ZZ_p^*$ e sia $r \in_R \left[ 0,\dots \frac{p-1}{2}\right]$ (\textit{esponente a 
caso per una possibile radice quadrata di un numero, presente nella prima metà}). Sia $y' = y \cdot g^{2x}$, 
dove $y'$ sarà un'altra istanza indipendente del problema dei 
residui quadratici, distribuita uniformemente in $\ZZ_p^*$, di cui conosciamo il logaritmo 
discreto. 
Se $2x + 2r < p - 1$ allora $z \cdot g^r$ è \texttt{PSQR} di $y \cdot g^{2r} 
\iff z$ è \texttt{PSQR} di $y$.
\end{theorem}
\[
    \textit{Sappiamo che } \sqrt{g^{2x} g^{2r}} = 
    \begin{cases}
      g^{x} g^{r}\\
      g^{x} g^{r} g^{\frac{p - 1}{2}}
  \end{cases}
\]
Con l'ipotesi \textcolor{red}{$2x + 2r < p - 1$} allora $g^{x+r}$ è la 
radice quadrata principale.

Questo perché moltiplicando $y \cdot g^{2r}$ abbiamo un'operazione \textbf{aritmetica} poiché 
gli esponenti sono sommati e non si ricade nella 
radice quadrata successiva, ovvero quella dopo la prima metà (\textit{l'esponente di 
$y$ è $2x$}). Moltiplicare quindi fa si che non si vada 
oltre $p - 1$, quindi il logaritmo discreto del prodotto è la somma dei logaritmi discreti;
andar oltre $p - 1$ significa tornare indietro, siccome siamo in un gruppo ciclico.

Dalla risposta al problema trasformato, sappiamo quindi calcolare la risposta al problema originale.

Ma come faccio a far si che $r$ scelta casualmente soddisfi le proprietà del teorema?
\begin{tcolorbox}
  Più $x$ è piccolo e più è probabile trovare un $r$ che soddisfa la proprietà del teorema.
\end{tcolorbox}
\begin{figure}[H]
  \centering 
  \begin{tikzpicture}
    % disegno un segmento orizzontale che va da 0 a (p-1)/2
    \draw[-] (0,0) -- (10,0);
    
    % segni sulla linea
    \foreach \x in {0,1,2,...,10} {
      \draw (\x,0.1) -- (\x,-0.1);
      
    }
    % etichette per gli estremi
    \node[above] at (0.5,0) {$\frac{x}{2}$};
    \node[below] at (0,-0.1) {0};
    \node[below] at (10,-0.1) {\(\frac{p-1}{2}\)};

    % parentesi graffa che racchiude i segmenti
    \draw [decorate,decoration={brace,amplitude=5pt, mirror},xshift=0pt,yshift=-24pt]
    (0,0) -- (10,0) node[midway,below=4pt] {$t$};
  \end{tikzpicture}
\end{figure}
Supponiamo che $\frac{x}{2}$ sia nel primo intervallo, la probabilità di trovare un $r$ dove 
$\frac{x}{2} + r$ non vada oltre $\frac{p - 1}{2}$, ovvero $\frac{t - 1}{t}$.
La probabilità che la risposta sia corretta è la combinazione della probabilità di 
soddisfacibilità del teorema, ovvero $\left( \frac{t - 1}{t}\right)$ e la probabilità
della risposta al problema trasformato, ovvero $\left( \frac{1}{2} + k^{-c}\right)$.

Questo perché qualche volta, dalla risposta al problema trasformato, non si riesce a
trovare la risposta al problema originale.

\begin{equation}
  \probP[\texttt{Esperimento corretto}] = \left( \frac{t - 1}{t}\right) \left( \frac{1}{2} + k^{-c}\right)
  \geq \frac{1}{2} + k^{-2c}
\end{equation}
Risolvendo la disequazione in $t$ riusciamo a capire qual è il limite inferiore a gli intervalli da utilizzare.

Dalla teoria dell'algebra sappiamo che si tratta di una disequazione polinomiale in $t$, come tale 
ha una soluzione polinomiale nelle costanti interne. La soluzione è quindi polinomiale nelle costanti 
presenti nell'equazione. Il limite inferiore è quindi polinomiale nelle costanti presenti nell'equazione.
\begin{equation}
  \frac{t - 1}{t} \geq \frac{1}{2} + k^{-c} \implies t \geq \frac{2}{1 + 2k^{-c}}
\end{equation}
Dividendo in una quantità di intervalli che è polinomiale in $k$ otteniamo un sistema tale per cui, se 
$x$ è nel primo intervallo, la probabilità di successo, ovvero di fornire una radice quadrata principale
è polinomialmente distante da $\frac{1}{2}$, di conseguenza abbiamo un algoritmo che risolve il problema
in tempo polinomiale.

Abbiamo quindi costruito un algoritmo che con probabilità $\frac{1}{2}$ calcola il logaritmo discreto
di un numero, a patto che la metà di tale logaritmo stia nel primo intervallo.
\subsubsection{Generalizzazione dell'intervallo}
Supponiamo di sapere che $\frac{x}{2}$ sia nell'intervallo $i-esimo$.
\begin{figure}[H]
  \centering 
  \begin{tikzpicture}
    % disegno un segmento orizzontale che va da 0 a (p-1)/2
    \draw[-] (0,0) -- (10,0);
    
    % segni sulla linea
    \foreach \x in {0,1,2,...,10} {
      \draw (\x,0.1) -- (\x,-0.1);
      
    }
    % etichette per gli estremi
    \node[above] at (4.5,0) {\textcolor{red}{$\frac{x}{2}$}};
    \node[above] at (3.5,0) {$\dots$};
    \node[above] at (5.5,0) {$\dots$};
    \node[below] at (0,-0.1) {0};
    \node[below] at (10,-0.1) {\(\frac{p-1}{2}\)};

    % parentesi graffa che racchiude i segmenti
    \draw [decorate,decoration={brace,amplitude=5pt, mirror},xshift=0pt,yshift=-24pt]
    (0,0) -- (10,0) node[midway,below=4pt] {$t$};
  \end{tikzpicture}
\end{figure}
Sappiamo che $y = g^x$ e che $i \frac{p - 1}{2t} \leq \frac{x}{2} < (i + 1) \frac{p - 1}{2t}$ 

\[
  g^{x'} = y \cdot g^{-\frac{p - 1}{t}i} \implies 
  g^{x'} = g^x \cdot g^{-\frac{p - 1}{t}i} \implies
  x' = x - \frac{p - 1}{t}i
\]
Quindi $x'$ sta nel primo intervallo, infatti da $\frac{x}{2}$ abbiamo tolto il punto d'inizio 
dell'intervallo $i-esimo$, togliendo tale quantità abbiamo ottenuto il risultato corrispondente al 
primo intervallo.
Ci siamo quindi ricondotti alla risoluzione di un problema relativo al calcolo della radice quadrata 
principale, ovvero il problema che abbiamo già risolto. Dopo aver risolto il problema
ci riconduciamo nuovamente al problema originale moltiplicando per $g^{\frac{p - 1}{t}i}$.

Non conoscendo l'intervallo in cui si trova $\frac{x}{2}$, possiamo provare a risolvere il problema
per tutti gli intervalli. Essendo quantità polinomiali in $k$.

\section{Bit Pseudocasuali}
Vogliamo costruire una sequenza di bit che sia equivalente ad un 
lancio di moneta. Se volessimo utilizzare all'interno di un programma
dei bit che sono casuali, avremmo bisogno di un processo di lancio di moneta 
all'interno del nostro calcolatore. All'interno del processo di lancio 
di moneta utilizzato c'è sempre un punto in cui ci sono scelte casuali da 
dover effettuare e tali scelte possono essere fatte da un processo realmente 
casuale. All'interno di un calcolatore non abbiamo un processo
realmente casuale, poiché i calcoli e le scelte sono deterministiche.
Il processo non è casuale, ma deterministico; ma appare come processo 
casuale agli occhi di chi lo osserva.

L'algoritmo deterministico 
esegue un'operazione sul \textbf{seme} e produce un output.
La generazione non può essere tale che la distribuzione di probabilità degli elementi 
sia uniforme, ma deve essere tale che produca una sequenza di bit tale per cui 
nessuno con potenza di calcolo probabilistica polinomiale sia in grado di 
capirne la distribuzione. Chiunque non si deve accorgere del fatto che 
non stiamo lavorando con sequenze casuali, ma con sequenze pseudocasuali.
\begin{figure}[H]
  \centering
  \begin{tikzpicture}
    \node[draw, rectangle, minimum width=4cm, minimum height=3cm] (A) at (0,0) {$\texttt{PRSG}$};
    \draw[->] (-5, 0) -- (-2, 0) node[midway, above] {$\textit{Seme}$};
    \draw[->] (2, 0) -- (5, 0) node[midway, above] {$b_0,b_1,\dots b_k$};
  \end{tikzpicture}
\end{figure}
Il seme $s$ viene scelto in maniera realmente casuale ed è corto ed ha $k$ bit e
l'output ha $l$ bit, normalmente con $l > k$. Il generatore di numeri pseudocasuali
possiamo vederlo come un \textbf{moltiplicatore di casualità}.
Nessun algoritmo probabilistico polinomiale sarà in grado di trovare 
una regolarità all'interno dell'output generato, nonostante ci sia.
\subsection{Generare bit pseudocasuali}
Avendo a disposizione la sequenza $b_0 b_1 ... b_l$, facciamo fatica ad 
indovinare il bit $b_{l+1}$, lo indoviniamo con un vantaggio $\frac{1}{2} + \epsilon$,
dove $\epsilon$ è più piccolo di qualsiasi polinomio.

\begin{equation}\label{eq:bit-pseudocasuali-uno}
  \bigg| \probP[s \in_R \{0,1\}^k; b_0 b_1 ... b_l \gets \mathcal{G}(s);
  b \gets \mathcal{A}(b_0 b_1 ... b_l); b = b_{l+1}] - \frac{1}{2} \bigg| \leq k^{-\omega(1)}
\end{equation}
La sequenza a disposizione la vedo come sequenza realmente casuale quando 
ogni singolo bit è risultato di un lancio di moneta. 

\begin{tcolorbox}
  Una sequenza di bit è pseudocasuale se nessun algoritmo probabilistico polinomiale
  è in grado di distinguere la sequenza generata da una sequenza realmente casuale.
\end{tcolorbox}

La definizione è un po' strana, perché dice che non siamo capaci di indovinare 
il bit successivo. Ci piacerebbe di più una definizione che dicesse che stiamo 
fornendo bit casuali nel momento in cui dalla sequenza di bit generata non siamo 
capaci di distinguerle da una sequenza realmente casuale.

Per definire un concetto simile dobbiamo essere in grado di dire che non c'è un algoritmo 
che data la sequenza veramente casuale e la sequenza pseudocasuale, non è in grado di
distinguere le due sequenze. Quindi descriviamo la probabilità che l'algoritmo $\mathcal{D}$ dia come risultato 1, 
ovvero che abbia riconosciuto la sequenza pseudocasuale come tale:

\[
  \mathcal{P}_k^{\mathcal{D},\mathcal{G}} = \probP[s \in_R \{0,1\}^k;
  b_0 b_1 ... b_l \gets \mathcal{G}(s); \mathcal{D}(b_0 b_1 ... b_l) = 1]
\]
Descriviamo la probabilità che l'algoritmo $\mathcal{D}$ dia come risultato 1, 
se prende in input una sequenza di bit realmente casuale:
\[
\mathcal{P}_k^{\mathcal{D},\mathcal{R}} = \probP[b_0 b_1 ... b_l \in_R \{0,1\}^{l+1}; \mathcal{D}(b_0 b_1 ... b_l) = 1]
\]
\begin{tcolorbox}
  Quindi la probabilità che il distinguisher restituisca $1$ nei due casi differisce di 
  una quantità più piccola di qualsiasi polinomio.
  
  \begin{equation}\label{eq:bit-pseudocasuali-due}
    \forall_{\mathcal{D} \in \texttt{PPT}} \quad \bigg| \mathcal{P}_k^{\mathcal{D},\mathcal{G}}
    - \mathcal{P}_k^{\mathcal{D},\mathcal{R}} \bigg| < k^{-\omega(1)}
  \end{equation}
\end{tcolorbox}
La formula appena descritta potrebbe essere una nuova definizione di generazione di bit pseudocasuali,
le due equazioni \ref{eq:bit-pseudocasuali-uno} e \ref{eq:bit-pseudocasuali-due} sono quindi equivalenti.
Disporre di definizioni equivalenti è utile perché ci permette di scegliere quella che ci è più comoda 
per il problema che stiamo affrontando.

\subsection{Generazione di bit pseudocasuali basati sul problema del logaritmo discreto} \label{subs:generazione-bit-pseudocasuali}
Fissiamo un numero primo $p$ e un generatore $g$ del gruppo $\mathbb{Z}_p^*$.
Prendiamo il seme $x \in_R \{0,\dots,p-1\}$, e definiamo:
\[
\begin{array}{llllll}
  a_0 & a_1 & a_2 & a_3 & \dots & a_l \\
  x & g^x & g^{g^x} & g^{g^{g^x}} & \dots & g^{g^{g^{\dots^{g^x}}}} \\
  b_0 & b_1 & b_2 & b_3 & \dots & b_l \\
\end{array}
\]
In maniera compatta:
\[
  \begin{cases}
    a_{i + 1} = g^{a_i} \\
    a_0 = x
  \end{cases}
\]
Generiamo elementi di $\mathbb{Z}_p^*$, dove il primo elemento è il seme 
e gli altri elementi sono ottenuti elevando $g$ all'elemento precedente.

A questo punto definisco i bit pseudocasuali come:
\[
  b_i = \begin{cases}
    0 & \text{se } a_i < \frac{p}{2} \\
    1 & \text{se } altrimenti
    \end{cases}
\]
Quindi l'hardcore predicate è $a_i < \frac{p - 1}{2}$
La sequenza pseudocasuale di ritorno sarà:
\[
\begin{array}{llllll}
  b_l & b_{l-1} & b_{l-2} & b_{l-3} & \dots & b_0 \\
\end{array}
\]
Utilizzando la definizione \ref{eq:bit-pseudocasuali-uno}, supponiamo per assurdo che esista 
un algoritmo in grado di predire il bit successivo con un vantaggio più grande 
di qualche polinomio.
I bit presi in input sono $b_l, ..., b_3$, quindi l'algoritmo $\mathcal{A}$ predice il 
bit $b_2$. ma $a_3$ è l'hardcore predicate di $b_2$, quindi non è possibile 
predire il bit $b_2$ con un vantaggio maggiore di qualche polinomio.
\[
  a^3 = g^{g^{g^x}} \quad \text{e} \quad a^2 = g^{g^x}
\]
La costruzione della sequenza 
$a_0, a_1, a_2, a_3, \dots, a_l$ e della sequenza $b_l, b_{l-1}, b_{l-2}, b_{l-3}, \dots, b_0$
è facilmente costruibile. Se i bit fossero stati restituiti in ordine sequenziale, 
$b_0, b_1, b_2, b_3, \dots, b_l$, non saremmo riusciti a far funzionare 
l'algoritmo perché i bit successivi della 
sequenza sarebbero stati il risultato di una funzione facilmente calcolabile.
La nostra costruzione si è basata sul fatto che la funzione da calcolare
sui semi è difficile al fine di dire che non riusciamo a prevedere il bit successivo.

Il fatto che la dimostrazione non funziona non implica che non sia possibile restituire 
i bit in ordine di enumerazione crescente.

La costruzione di funzionamento con i bit restituiti alla rovescia si basa sul fatto che
conoscendo i semi non siamo in grado di calcolare $b_2$, se non so calcolare $b_2$ 
conoscendo i semi, a maggior ragione non sono in grado di calcolarlo conoscendo
i bit, ma tale dimostrazione non mi permette di catturare tale concetto.

Prendendo in considerazione la formula \ref{eq:bit-pseudocasuali-due}, se i bit restituiti
alla rovescia sono indistinguibili da una sequenza casuale, allora anche i bit restituiti
in ordine crescente sono indistinguibili da una sequenza casuale.

\begin{tcolorbox}
  Se $b_l, \dots, b_0$ soddisfa la definizione \ref{eq:bit-pseudocasuali-due}, allora soddisfa 
  la definizione \ref{eq:bit-pseudocasuali-uno}. Quindi
  $b_l, \dots, b_0$ è indistinguibile da una sequenza casuale e possiamo concludere che
  $b_0, \dots, b_l$ è indistinguibile da una sequenza casuale.
\end{tcolorbox}
Sia un distinguisher $\mathcal{D}$ per $b_l, \dots, b_0$, quindi $\mathcal{D}$ prende in input
$b_l, \dots, b_0$ e restituisce $1$ se la sequenza è pseudocasuale e $0$ se la sequenza è casuale.
Sia $\mathcal{D}'$ un distinguisher per $b_l, \dots, b_0$, quindi $\mathcal{D}'$ prende in input
$b_0, \dots, b_l$ e restituisce $\mathcal{D}(b_0, \dots, b_l)$, quindi rovescia la sequenza e 
restituisce il risultato di $\mathcal{D}$ basata sulla sequenza rovesciata.

Se $\mathcal{D}$ distingue $b_l, \dots, b_0$ allora $\mathcal{D}'$ distingue $b_0, \dots, b_l$ 
e viceversa. Quindi distinguere i bit in ordine di enumerazione crescente è equivalente a distinguere i bit
in ordine di enumerazione decrescente.

Restituire i bit alla rovescia è svantaggioso perché non è possibile calcolare il bit successivo, mentre 
restituire i bit in ordine di enumerazione crescente permette di andare avanti con la sequenza.
\subsection{Distinguere equivale a prevedere}
\begin{theorem}
  \begin{enumerate}
    \item Sia distinguere equivalente a:
    \begin{equation}
      \forall_{\mathcal{D} \in \texttt{PPT}} \quad \bigg| \mathcal{P}_k^{\mathcal{D},\mathcal{G}}
      - \mathcal{P}_k^{\mathcal{D},\mathcal{R}} \bigg| < k^{-\omega(1)}
    \end{equation}
    \item Sia prevedere equivalente a:
    \begin{equation}
      \bigg| \probP[s \in_R \{0,1\}^k; b_0 b_1 ... b_l \gets \mathcal{G}(s);
      b \gets \mathcal{A}(b_0 b_1 ... b_l); b = b_{l+1}] - \frac{1}{2} \bigg| \leq k^{-\omega(1)}
    \end{equation}
  \end{enumerate}
  Allora:
  \begin{itemize}
    \item $1 \implies 2$
    \item $2 \implies 1$
  \end{itemize}

\end{theorem}
\begin{proof}
  ($1 \implies 2$) Su input $b_0, \dots, b_{l - 1}$ indoviniamo $b_l$ con vantaggio polinomiale,
  ovvero:
  \[
    \probP[\mathcal{A}(b_0, \dots, b_{l-1}) = b_l] - \frac{1}{2} \geq k^{-c}
  \]
  \[
    \probP[\mathcal{A}(b_0, \dots, b_{l-1}) = b_l] \geq \frac{1}{2} + k^{-c}
  \]
  e possiamo affermarlo senza perdita di generalità. Se vogliamo dimostrare che la proprietà 
  ($2$) vale, ovvero che violando la proprietà ($1$) si viola anche la proprietà ($2$), 
  allora possiamo costruire un distinguisher.
  \[
    \mathcal{D}(b_0, \dots, b_l) = \begin{cases}
      1 & \text{se } \mathcal{A}(b_0, \dots, b_{l-1}) = b_l \\
      0 & \text{altrimenti}
      \end{cases}
  \]
  Sappiamo che la probabilità che l'algoritmo $\mathcal{A}$ indovini 
  un bit scelto in maniera veramente casuale e indipendente da $b_0, \dots, b_{l - 1}$ 
  è esattamente $\frac{1}{2}$ (\textit{one-time pad}), ovvero:
  \[\mathcal{P}_k^{\mathcal{D},\mathcal{U}} = \frac{1}{2}\]
  Mentre, la probabilità che $\mathcal{A}$ con in input $b_0, \dots, b_{l - 1}$ indovini
  $b_l$ è maggiore di $\frac{1}{2} + k^{-c}$, ovvero:
  \[\mathcal{P}_k^{\mathcal{D},\mathcal{G}} \geq \frac{1}{2} + k^{-c}\]

  Di conseguenza:
  \[
    \mathcal{P}_k^{\mathcal{D},\mathcal{G}} - \mathcal{P}_k^{\mathcal{D},\mathcal{U}} \geq \frac{1}{2} + k^{-c} - \frac{1}{2} = k^{-c}
  \]
  Se esiste l'attaccante per la proprietà ($1$), allora violiamo la proprietà ($2$).

  ($2 \implies 1$) Disponiamo di una sequenza di bit $b_1, \dots, b_l$ generata da $\mathcal{G}$ e 
  una sequenza di bit $r_1, \dots, r_l$ generata in maniera casuale e indipendente da $\mathcal{G}$, supponiamo 
  che $\mathcal{D}$ sia un distinguisher per $b_1, \dots, b_l$ e $r_1, \dots, r_l$.
  
  Sappiamo che:
  \begin{align*}
    \begin{array}{r c lllllllll}
    \mathcal{S}_0 & = & b^1 &\dots &b^{i-1} & b^i &b^{i+1} & b^{i+2} &\dots &b^l \\
    \mathcal{S}_{l}& = & r^1 &\dots &r^{i-1} & r^i &r^{i+1} & r^{i+2} &\dots &r^l \\
    \end{array}
  \end{align*}
  Con la tecnica di 
  interpolazione, supponendo che le sequenze differiscano di un solo bit 
  (\textit{come dimostrato nella sezione \ref{sec:distinguisher}}), siamo 
  in grado di affermare che:
  \[
    \bigg| \mathcal{P}_k^{\mathcal{D},\mathcal{G}} - \mathcal{P}_k^{\mathcal{D},\mathcal{R}} \bigg| \geq \frac{1}{2} - k^{-c}
  \]
  \[
    \bigg| \sum_{i = 0}^{l}(\mathcal{P}_i - \mathcal{P}_{i - 1}) \bigg| \leq \sum_{i = 0}^{l} \bigg| (\mathcal{P}_i - \mathcal{P}_{i - 1}) \bigg|
  \]
  Quindi
  \[
    \exists i \in \{0, \dots, l\} \quad \bigg| \mathcal{P}_i - \mathcal{P}_{i - 1} \bigg| \geq \frac{k^{-c}}{l} = k^{-c'}
  \]
  \begin{align*}
    \begin{array}{r c lllllllll}
    \mathcal{S}_i & = & b^1 &\dots &b^{i-1} & b^i &\textcolor{red}{r^{i+1}} & r^{i+2} &\dots &r^l \\
    \mathcal{S}_{i+1}& = & b^1 &\dots &b^{i-1} & b^i &\textcolor{red}{b^{i+1}} & r^{i+2} &\dots &b^l \\
    \end{array}
  \end{align*}
  In qualche modo il distinguisher si comporta diversamente quando i bit differiscono di un solo bit.

  Vorremmo cercare di costruire un algoritmo che indovini i bit pseudocasuali.
  Sia
  \[
    \mathcal{F} = \mathcal{D}(b_0, \dots, b_{i}, r_{i+1}, \dots, r_l)
  \]
  \[
    \mathcal{A}(b_1,...,b_l) = 
    \begin{cases}
      r_{i+1} & \text{se } \mathcal{F} = 1 \\
      \bar{r}_{i+1} & \text{se } \mathcal{F} = 0
    \end{cases}
  \]
  Utilizziamo quindi il distinguisher per indovinare il bit successivo.

  Senza perdita di generalità $\mathcal{P}_{i+1} - \mathcal{P}_i \geq k^{-c'}$,
  supponiamo che la probabilità che $\mathcal{D}$ restituisca $0$ su input $b_1, \dots, b_i, \bar{b}_{i+1}, \dots, r_l$ sia $x$,
  allora la probabilità che $\mathcal{A}$ indovini il bit successivo è:
  \[
    \probP[\mathcal{A}(b_1,...,b_l) = b_{i+1}] = \probP[r_{i+1} = b_{i+1}] \cdot \mathcal{P}_{i+1} + \probP[r_{i+1} = \bar{b}_{i+1}] \cdot x
  \]
  \[
    \frac{1}{2}\cdot \mathcal{P}_{i+1} + \frac{1}{2}\cdot x
  \]
  Proviamo attualmente a cercare una relazione linearmente indipendente con $\mathcal{P}_{i+1}$. Il valore 
  equivale alla probabilità che il distinguisher restituisca $1$ su input $b_1, \dots, b_i, r_{i+1}, \dots, r_l$.
  \begin{align*}
    \mathcal{P}_i &= \probP[\textit{il bit i-esimo} = b_{i+1}] \cdot \probP[\mathcal{D} \textit{ dia 1 su } b_1, \dots, b_i, b_{i+1}, \dots, r_l] +\\
    &+ \probP[\textit{il bit i-esimo} = \bar{b}_{i+1}] \cdot \probP[\mathcal{D} \textit{ dia 0 su } b_1, \dots, b_i, \bar{b}_{i+1}, \dots, r_l] =\\
    &= \frac{1}{2} \cdot \mathcal{P}_{i+1} + \frac{1}{2} \cdot (1 - x) = \frac{1}{2} \cdot \mathcal{P}_{i+1} + \frac{1}{2} - \frac{1}{2} \cdot x
  \end{align*}
  Ricaviamo quindi che:
  \[
    x = \mathcal{P}_{i+1} + 1 - 2 \cdot \mathcal{P}_i
  \]
  Sostituendo $x$ nell'equazione precedente:
  \begin{align*}
    &= \frac{1}{2} \cdot \mathcal{P}_{i+1} + \frac{1}{2} \cdot \mathcal{P}_{i+1} + \frac{1}{2} - \mathcal{P}_i\\
    &= \frac{1}{2} + \mathcal{P}_{i+1} - \mathcal{P}_i  
  \end{align*}
  Ma sappiamo che $\mathcal{P}_{i+1} - \mathcal{P}_i \geq k^{-c'}$, quindi:
  \[
    \frac{1}{2} + \mathcal{P}_{i+1} - \mathcal{P}_i \geq \frac{1}{2} + k^{-c'}
  \]
\end{proof}
\section{Blum Blum Shub}
Si tratta di un algoritmo di generazione di numeri pseudocasuali 
basato sulla difficoltà del calcolo di radici quadratiche in $\mathbb{Z}_n^*$.

La funzione di elevamento al quadrato è una funzione \textit{one-way trapdoor} in $\mathbb{Z}_{p\cdot q}^*$,
poiché la conoscenza di $p$ e $q$, ovvero la fattorizzazione di $n$,
permette di calcolare la radice quadrata in tempo polinomiale.
Sappiamo inoltre che ogni funzione \textit{one-way} ammette un \textit{hardcore predicate}, infatti:
\[
  \texttt{LSB}(x) = (\sqrt{x})
\]
Il problema è che $x$ ha più di una radice quadrata, che radice bisogna selezionare?
\begin{theorem}[Primi di Blum]
  Se $p$ e $q$ sono primi congrui a $3 \mod 4$ allora per ogni quadrato $x$ 
  esiste una sola radice quadrata $y$ tale che $y \equiv \sqrt{x} \mod n$.
\end{theorem}

Sia $n = p \cdot q$ con $p$ e $q$ primi di Blum scelti casualmente, sia 
$x \in_R \mathcal{Z}^*$, allora
\begin{align*}
  \begin{array}{lllll}
    a_1 & a_2 & a_3 & \dots & a_l \\
    x^2 & (x^2)^2 & ((x^2)^2)^2 & \dots & x^(2^l)
  \end{array}
\end{align*}
ovvero
\[
  \begin{cases}
    x_i = x^2\\
    x_{i+1} = a_i^2
  \end{cases}  
\]
Definiamo quindi $b_i = \texttt{LSB}(a_i)$, quindi:
\begin{align*}
  \begin{array}{lllll}
    a_1 & a_2 & a_3 & \dots & a_l \\
    x^2 & (x^2)^2 & ((x^2)^2)^2 & \dots & x^(2^l)\\
    b_1 & b_2 & b_3 & \dots & b_l
  \end{array}
\end{align*}
Dove il valore di ritorno sarà la sequenza $b_l, b_{l-1}, \dots, b_1$.
Utilizzando il principio spiegato nella sezione precedente (\ref{subs:generazione-bit-pseudocasuali}) 
possiamo restituire i bit in ordine di enumerazione crescente, ovvero $b_1, b_2, \dots, b_l$.

La differenza è che se il numero $n$ è conosciuto da tutti, allora è possibile restituire i bit solamente 
nell'ordine $b_1, b_2, \dots, b_l$, altrimenti in qualsiasi ordine.

\section{Crittosistema di Blum Goldwasser}
L'idea è stata di utilizzare un crittosistema dato dai soliti tre algoritmi, quello di generazione 
della chiave, quello di codifica e quello di decodifica.
La chiave è data da $p, q$ primi di Blum con $k$ bit, $n = p \cdot q$, dove la chiave pubblica 
è data da $n$ e la chiave privata è data da $p$ e $q$.
Chi possiede la chiave segreta riesce a calcolare radici quadrate in modulo $n$. L'algoritmo 
di codifica è dato da un $x\in_R \mathbb{Z}_n^*$, e attraverso blum blum shub, generiamo 
$l$ bit pseudocausali $b_1, \dots, b_l$. Supponiamo di avere un messaggio $m$ di lunghezza $l$,
allora il messaggio cifrato sarà:
\[
  c_1, \dots, c_l, x^{2^{l+1}} = m_1 \oplus b_1, \dots, m_l \oplus b_l
\]
Dove $x^{2^{l+1}}$ è il primo elemento della sequenza generata da blum blum shub.

Sostanzialmente il messaggio viene cifrato attraverso one time pad, 
con una chiave non realmente casuale, ma con numeri pseudocasuali mediante un seme.

Se la sequenza $b_1, \dots, b_l$ è casuale, allora il cyphertext non contiene 
alcuna informazione sul plaintext, quindi il crittosistema è sicuro.

Supponendo per assurdo che dal ciphertext sia possibile ricavare informazioni sul plaintext,
allora si riesce a distinguere qualunque coppia di messaggi, ma sappiamo che non lo si riesce a 
fare in presenza di una chiave realmente casuale. Ma nel caso specifico del nostro crittosistema, 
si dovrebbe riuscire a distinguere sul generato di bit pseudocausali, ma questo è assurdo 
poiché non siamo in grado di distinguere una sequenza pseudocasuale da una casuale.

La decodifica ricava $x^2$, infatti, data la conoscenza della fattorizzazione di $n$, 
è possibile calcolare $\sqrt{x^{2^{l+1}}} = x^{2^l}$, e così via. Si ricava quindi
$x^{2^l}, x^{2^{l-1}}, \dots, x^2$ e si calcolano i bit $b_1, \dots, b_l$. Attraverso 
questi bit è possibile decifrare il messaggio con l'operazione inversa:
\[
  m_i = c_i \oplus b_i
\]

L'algoritmo in questione effettua il calcolo di $l$ quadrati,
con un costo di $\Theta(n^2)$ per ciascuna operazione di elevamento
al quadrato. Pertanto, il costo totale dell'algoritmo è $\Theta(l \cdot n^2)$.
Questo algoritmo è uno stream cypher, a differenza di \texttt{RSA}, che è invece
un block cypher. Nonostante questa differenza nel metodo di cifratura, le complessità
computazionali di entrambi gli algoritmi risultano essere simili. La distinzione principale
risiede nel fatto che l'algoritmo di Blum Goldwasser gode di una dimostrazione di sicurezza,
dimostrabilità che manca invece in \texttt{RSA}.